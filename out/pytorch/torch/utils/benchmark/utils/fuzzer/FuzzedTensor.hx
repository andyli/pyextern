/* This file is generated, do not edit! */
package torch.utils.benchmark.utils.fuzzer;
@:pythonImport("torch.utils.benchmark.utils.fuzzer", "FuzzedTensor") extern class FuzzedTensor {
	public function __class__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Implement delattr(self, name).
	**/
	public function __delattr__(name:Dynamic):Dynamic;
	static public var __dict__ : Dynamic;
	/**
		Default dir() implementation.
	**/
	public function __dir__():Dynamic;
	static public var __doc__ : Dynamic;
	/**
		Return self==value.
	**/
	public function __eq__(value:Dynamic):Dynamic;
	/**
		Default object formatter.
	**/
	public function __format__(format_spec:Dynamic):Dynamic;
	/**
		Return self>=value.
	**/
	public function __ge__(value:Dynamic):Dynamic;
	/**
		Return getattr(self, name).
	**/
	public function __getattribute__(name:Dynamic):Dynamic;
	/**
		Return self>value.
	**/
	public function __gt__(value:Dynamic):Dynamic;
	/**
		Return hash(self).
	**/
	public function __hash__():Dynamic;
	/**
		Args:
		    name:
		        A string identifier for the generated Tensor.
		    size:
		        A tuple of integers or strings specifying the size of the generated
		        Tensor. String values will replaced with a concrete int during the
		        generation process, while ints are simply passed as literals.
		    steps:
		        An optional tuple with the same length as `size`. This indicates
		        that a larger Tensor should be allocated, and then sliced to
		        produce the generated Tensor. For instance, if size is (4, 8)
		        and steps is (1, 4), then a tensor `t` of size (4, 32) will be
		        created and then `t[:, ::4]` will be used. (Allowing one to test
		        Tensors with strided memory.)
		    probability_contiguous:
		        A number between zero and one representing the chance that the
		        generated Tensor has a contiguous memory layout. This is achieved by
		        randomly permuting the shape of a Tensor, calling `.contiguous()`,
		        and then permuting back. This is applied before `steps`, which can
		        also cause a Tensor to be non-contiguous.
		    min_elements:
		        The minimum number of parameters that this Tensor must have for a
		        set of parameters to be valid. (Otherwise they are resampled.)
		    max_elemnts:
		        Like `min_elements`, but setting an upper bound.
		    max_allocation_bytes:
		        Like `max_elements`, but for the size of Tensor that must be
		        allocated prior to slicing for `steps` (if applicable). For
		        example, a FloatTensor with size (1024, 1024) and steps (4, 4)
		        would have 1M elements, but would require a 64 MB allocation.
		    dim_parameter:
		        The length of `size` and `steps` will be truncated to this value.
		        This allows Tensors of varying dimensions to be generated by the
		        Fuzzer.
		    dtype:
		        The PyTorch dtype of the generated Tensor.
		    cuda:
		        Whether to place the Tensor on a GPU.
		    tensor_constructor:
		        Callable which will be used instead of the default Tensor
		        construction method. This allows the author to enforce properties
		        of the Tensor (e.g. it can only have certain values). The dtype and
		        concrete shape of the Tensor to be created will be passed, and
		        concrete values of all parameters will be passed as kwargs. Note
		        that transformations to the result (permuting, slicing) will be
		        performed by the Fuzzer; the tensor_constructor is only responsible
		        for creating an appropriately sized Tensor.
	**/
	@:native("__init__")
	public function ___init__(name:Dynamic, size:Dynamic, ?steps:Dynamic, ?probability_contiguous:Dynamic, ?min_elements:Dynamic, ?max_elements:Dynamic, ?max_allocation_bytes:Dynamic, ?dim_parameter:Dynamic, ?roll_parameter:Dynamic, ?dtype:Dynamic, ?cuda:Dynamic, ?tensor_constructor:Dynamic):Dynamic;
	/**
		Args:
		    name:
		        A string identifier for the generated Tensor.
		    size:
		        A tuple of integers or strings specifying the size of the generated
		        Tensor. String values will replaced with a concrete int during the
		        generation process, while ints are simply passed as literals.
		    steps:
		        An optional tuple with the same length as `size`. This indicates
		        that a larger Tensor should be allocated, and then sliced to
		        produce the generated Tensor. For instance, if size is (4, 8)
		        and steps is (1, 4), then a tensor `t` of size (4, 32) will be
		        created and then `t[:, ::4]` will be used. (Allowing one to test
		        Tensors with strided memory.)
		    probability_contiguous:
		        A number between zero and one representing the chance that the
		        generated Tensor has a contiguous memory layout. This is achieved by
		        randomly permuting the shape of a Tensor, calling `.contiguous()`,
		        and then permuting back. This is applied before `steps`, which can
		        also cause a Tensor to be non-contiguous.
		    min_elements:
		        The minimum number of parameters that this Tensor must have for a
		        set of parameters to be valid. (Otherwise they are resampled.)
		    max_elemnts:
		        Like `min_elements`, but setting an upper bound.
		    max_allocation_bytes:
		        Like `max_elements`, but for the size of Tensor that must be
		        allocated prior to slicing for `steps` (if applicable). For
		        example, a FloatTensor with size (1024, 1024) and steps (4, 4)
		        would have 1M elements, but would require a 64 MB allocation.
		    dim_parameter:
		        The length of `size` and `steps` will be truncated to this value.
		        This allows Tensors of varying dimensions to be generated by the
		        Fuzzer.
		    dtype:
		        The PyTorch dtype of the generated Tensor.
		    cuda:
		        Whether to place the Tensor on a GPU.
		    tensor_constructor:
		        Callable which will be used instead of the default Tensor
		        construction method. This allows the author to enforce properties
		        of the Tensor (e.g. it can only have certain values). The dtype and
		        concrete shape of the Tensor to be created will be passed, and
		        concrete values of all parameters will be passed as kwargs. Note
		        that transformations to the result (permuting, slicing) will be
		        performed by the Fuzzer; the tensor_constructor is only responsible
		        for creating an appropriately sized Tensor.
	**/
	public function new(name:Dynamic, size:Dynamic, ?steps:Dynamic, ?probability_contiguous:Dynamic, ?min_elements:Dynamic, ?max_elements:Dynamic, ?max_allocation_bytes:Dynamic, ?dim_parameter:Dynamic, ?roll_parameter:Dynamic, ?dtype:Dynamic, ?cuda:Dynamic, ?tensor_constructor:Dynamic):Void;
	/**
		This method is called when a class is subclassed.
		
		The default implementation does nothing. It may be
		overridden to extend subclasses.
	**/
	public function __init_subclass__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Return self<=value.
	**/
	public function __le__(value:Dynamic):Dynamic;
	/**
		Return self<value.
	**/
	public function __lt__(value:Dynamic):Dynamic;
	static public var __module__ : Dynamic;
	/**
		Return self!=value.
	**/
	public function __ne__(value:Dynamic):Dynamic;
	/**
		Create and return a new object.  See help(type) for accurate signature.
	**/
	static public function __new__(?args:python.VarArgs<Dynamic>, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
	/**
		Helper for pickle.
	**/
	public function __reduce__():Dynamic;
	/**
		Helper for pickle.
	**/
	public function __reduce_ex__(protocol:Dynamic):Dynamic;
	/**
		Return repr(self).
	**/
	public function __repr__():Dynamic;
	/**
		Implement setattr(self, name, value).
	**/
	public function __setattr__(name:Dynamic, value:Dynamic):Dynamic;
	/**
		Size of object in memory, in bytes.
	**/
	public function __sizeof__():Dynamic;
	/**
		Return str(self).
	**/
	public function __str__():Dynamic;
	/**
		Abstract classes can override this to customize issubclass().
		
		This is invoked early on by abc.ABCMeta.__subclasscheck__().
		It should return True, False or NotImplemented.  If it returns
		NotImplemented, the normal algorithm is used.  Otherwise, it
		overrides the normal algorithm (and the outcome is cached).
	**/
	public function __subclasshook__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		list of weak references to the object (if defined)
	**/
	public var __weakref__ : Dynamic;
	public function _get_size_and_steps(params:Dynamic):Dynamic;
	public function _make_tensor(params:Dynamic, state:Dynamic):Dynamic;
	static public function default_tensor_constructor(size:Dynamic, dtype:Dynamic, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
	public var name : Dynamic;
	public function satisfies_constraints(params:Dynamic):Dynamic;
}