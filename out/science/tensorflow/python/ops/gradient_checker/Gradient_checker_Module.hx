/* This file is generated, do not edit! */
package tensorflow.python.ops.gradient_checker;
@:pythonImport("tensorflow.python.ops.gradient_checker") extern class Gradient_checker_Module {
	static public var __builtins__ : Dynamic;
	static public var __cached__ : Dynamic;
	static public var __doc__ : Dynamic;
	static public var __file__ : Dynamic;
	static public var __loader__ : Dynamic;
	static public var __name__ : Dynamic;
	static public var __package__ : Dynamic;
	static public var __spec__ : Dynamic;
	/**
		Returns a node to compute gradient of y wrt x.
	**/
	static public function _compute_dx_and_dy(x:Dynamic, y:Dynamic, y_shape:Dynamic):Dynamic;
	static public function _compute_error(grad:Dynamic):Dynamic;
	/**
		Computes the theoretical and numerical jacobian.
	**/
	static public function _compute_gradient(x:Dynamic, x_shape:Dynamic, dx:Dynamic, y:Dynamic, y_shape:Dynamic, dy:Dynamic, ?x_init_value:Dynamic, ?delta:Dynamic, ?extra_feed_dict:Dynamic):Dynamic;
	/**
		Compute gradients for a list of x values.
	**/
	static public function _compute_gradient_list(x:Dynamic, x_shape:Dynamic, y:Dynamic, y_shape:Dynamic, ?x_init_value:Dynamic, ?delta:Dynamic, ?init_targets:Dynamic, ?extra_feed_dict:Dynamic):Dynamic;
	/**
		Computes the numeric Jacobian for dy/dx.
		
		Computes the numeric Jacobian by slightly perturbing the inputs and
		measuring the differences on the output.
		
		Args:
		  x: the tensor "x".
		  x_shape: the dimensions of x as a tuple or an array of ints.
		  x_data: a numpy array as the input data for x
		  y: the tensor "y".
		  y_shape: the dimensions of y as a tuple or an array of ints.
		  delta: the amount of perturbation we give to the input
		  extra_feed_dict: dict that allows fixing specified tensor values
		    during the jacobian calculation.
		
		Returns:
		  A 2-d numpy array representing the Jacobian for dy/dx. It has "x_size" rows
		  and "y_size" columns where "x_size" is the number of elements in x and
		  "y_size" is the number of elements in y.
	**/
	static public function _compute_numeric_jacobian(x:Dynamic, x_shape:Dynamic, x_data:Dynamic, y:Dynamic, y_shape:Dynamic, delta:Dynamic, extra_feed_dict:Dynamic):Dynamic;
	/**
		Computes the theoretical Jacobian for dy/dx.
		
		Computes the theoretical Jacobian using the ops generated by
		compute_gradient().
		
		Args:
		  x: the tensor "x".
		  x_shape: the dimensions of x as a tuple or an array of ints.
		  x_data: a numpy parray as the input data for x
		  dy: the tensor "dy".
		  dy_shape: the dimensions of dy as a tuple or an array of ints.
		  dx: Tensor or IndexedSlices representing dx
		  extra_feed_dict: dict that allows fixing specified tensor values
		    during the jacobian calculation.
		
		Returns:
		  A 2-d numpy array representing the Jacobian for dy/dx. It has "x_size" rows
		  and "dy_size" columns where "x_size" is the number of elements in x and
		  "dy_size" is the number of elements in dy.
		
		Raises:
		  ValueError: If `dy` is empty but the gradient is nonzero.
	**/
	static public function _compute_theoretical_jacobian(x:Dynamic, x_shape:Dynamic, x_data:Dynamic, dy:Dynamic, dy_shape:Dynamic, dx:Dynamic, extra_feed_dict:Dynamic):Dynamic;
	static public function _extra_feeds(extra_feed_dict:Dynamic, new_feeds:Dynamic):Dynamic;
	static public function _product(t:Dynamic):Dynamic;
	static public var absolute_import : Dynamic;
	/**
		Computes and returns the theoretical and numerical Jacobian. (deprecated)
		
		Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
		Instructions for updating:
		Use tf.test.compute_gradient in 2.0, which has better support for functions. Note that the two versions have different usage, so code change is needed.
		
		If `x` or `y` is complex, the Jacobian will still be real but the
		corresponding Jacobian dimension(s) will be twice as large.  This is required
		even if both input and output is complex since TensorFlow graphs are not
		necessarily holomorphic, and may have gradients not expressible as complex
		numbers.  For example, if `x` is complex with shape `[m]` and `y` is complex
		with shape `[n]`, each Jacobian `J` will have shape `[m * 2, n * 2]` with
		
		    J[:m, :n] = d(Re y)/d(Re x)
		    J[:m, n:] = d(Im y)/d(Re x)
		    J[m:, :n] = d(Re y)/d(Im x)
		    J[m:, n:] = d(Im y)/d(Im x)
		
		Args:
		  x: a tensor or list of tensors
		  x_shape: the dimensions of x as a tuple or an array of ints. If x is a list,
		  then this is the list of shapes.
		  y: a tensor
		  y_shape: the dimensions of y as a tuple or an array of ints.
		  x_init_value: (optional) a numpy array of the same shape as "x"
		    representing the initial value of x. If x is a list, this should be a list
		    of numpy arrays.  If this is none, the function will pick a random tensor
		    as the initial value.
		  delta: (optional) the amount of perturbation.
		  init_targets: list of targets to run to initialize model params.
		  extra_feed_dict: dict that allows fixing specified tensor values
		    during the Jacobian calculation.
		
		Returns:
		  Two 2-d numpy arrays representing the theoretical and numerical
		  Jacobian for dy/dx. Each has "x_size" rows and "y_size" columns
		  where "x_size" is the number of elements in x and "y_size" is the
		  number of elements in y. If x is a list, returns a list of two numpy arrays.
	**/
	static public function compute_gradient(x:Dynamic, x_shape:Dynamic, y:Dynamic, y_shape:Dynamic, ?x_init_value:Dynamic, ?delta:Dynamic, ?init_targets:Dynamic, ?extra_feed_dict:Dynamic):Dynamic;
	/**
		Computes the gradient error. (deprecated)
		
		Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
		Instructions for updating:
		Use tf.test.compute_gradient in 2.0, which has better support for functions. Note that the two versions have different usage, so code change is needed.
		
		Computes the maximum error for dy/dx between the computed Jacobian and the
		numerically estimated Jacobian.
		
		This function will modify the tensors passed in as it adds more operations
		and hence changing the consumers of the operations of the input tensors.
		
		This function adds operations to the current session. To compute the error
		using a particular device, such as a GPU, use the standard methods for
		setting a device (e.g. using with sess.graph.device() or setting a device
		function in the session constructor).
		
		Args:
		  x: a tensor or list of tensors
		  x_shape: the dimensions of x as a tuple or an array of ints. If x is a list,
		  then this is the list of shapes.
		  y: a tensor
		  y_shape: the dimensions of y as a tuple or an array of ints.
		  x_init_value: (optional) a numpy array of the same shape as "x"
		    representing the initial value of x. If x is a list, this should be a list
		    of numpy arrays.  If this is none, the function will pick a random tensor
		    as the initial value.
		  delta: (optional) the amount of perturbation.
		  init_targets: list of targets to run to initialize model params.
		  extra_feed_dict: dict that allows fixing specified tensor values
		    during the Jacobian calculation.
		
		Returns:
		  The maximum error in between the two Jacobians.
	**/
	static public function compute_gradient_error(x:Dynamic, x_shape:Dynamic, y:Dynamic, y_shape:Dynamic, ?x_init_value:Dynamic, ?delta:Dynamic, ?init_targets:Dynamic, ?extra_feed_dict:Dynamic):Dynamic;
	static public var division : Dynamic;
	static public var print_function : Dynamic;
	static public function tf_export(?args:python.VarArgs<Dynamic>, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
}