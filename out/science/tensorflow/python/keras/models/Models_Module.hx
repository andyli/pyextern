/* This file is generated, do not edit! */
package tensorflow.python.keras.models;
@:pythonImport("tensorflow.python.keras.models") extern class Models_Module {
	/**
		`Input()` is used to instantiate a Keras tensor.
		
		A Keras tensor is a symbolic tensor-like object,
		which we augment with certain attributes that allow us to build a Keras model
		just by knowing the inputs and outputs of the model.
		
		For instance, if `a`, `b` and `c` are Keras tensors,
		it becomes possible to do:
		`model = Model(input=[a, b], output=c)`
		
		Args:
		    shape: A shape tuple (integers), not including the batch size.
		        For instance, `shape=(32,)` indicates that the expected input
		        will be batches of 32-dimensional vectors. Elements of this tuple
		        can be None; 'None' elements represent dimensions where the shape is
		        not known.
		    batch_size: optional static batch size (integer).
		    name: An optional name string for the layer.
		        Should be unique in a model (do not reuse the same name twice).
		        It will be autogenerated if it isn't provided.
		    dtype: The data type expected by the input, as a string
		        (`float32`, `float64`, `int32`...)
		    sparse: A boolean specifying whether the placeholder to be created is
		        sparse. Only one of 'ragged' and 'sparse' can be True. Note that,
		        if `sparse` is False, sparse tensors can still be passed into the
		        input - they will be densified with a default value of 0.
		    tensor: Optional existing tensor to wrap into the `Input` layer.
		        If set, the layer will use the `tf.TypeSpec` of this tensor rather
		        than creating a new placeholder tensor.
		    ragged: A boolean specifying whether the placeholder to be created is
		        ragged. Only one of 'ragged' and 'sparse' can be True. In this case,
		        values of 'None' in the 'shape' argument represent ragged dimensions.
		        For more information about RaggedTensors, see
		        [this guide](https://www.tensorflow.org/guide/ragged_tensors).
		    type_spec: A `tf.TypeSpec` object to create the input placeholder from.
		        When provided, all other args except name must be None.
		    **kwargs: deprecated arguments support. Supports `batch_shape` and
		        `batch_input_shape`.
		
		Returns:
		  A `tensor`.
		
		Example:
		
		```python
		# this is a logistic regression in Keras
		x = Input(shape=(32,))
		y = Dense(16, activation='softmax')(x)
		model = Model(x, y)
		```
		
		Note that even if eager execution is enabled,
		`Input` produces a symbolic tensor-like object (i.e. a placeholder).
		This symbolic tensor-like object can be used with lower-level
		TensorFlow ops that take tensors as inputs, as such:
		
		```python
		x = Input(shape=(32,))
		y = tf.square(x)  # This op will be treated like a layer
		model = Model(x, y)
		```
		
		(This behavior does not work for higher-order TensorFlow APIs such as
		control flow and being directly watched by a `tf.GradientTape`).
		
		However, the resulting model will not track any variables that were
		used as inputs to TensorFlow ops. All variable usages must happen within
		Keras layers to make sure they will be tracked by the model's weights.
		
		The Keras Input can also create a placeholder from an arbitrary `tf.TypeSpec`,
		e.g:
		
		```python
		x = Input(type_spec=tf.RaggedTensorSpec(shape=[None, None],
		                                        dtype=tf.float32, ragged_rank=1))
		y = x.values
		model = Model(x, y)
		```
		When passing an arbitrary `tf.TypeSpec`, it must represent the signature of an
		entire batch instead of just one example.
		
		Raises:
		  ValueError: If both `sparse` and `ragged` are provided.
		  ValueError: If both `shape` and (`batch_input_shape` or `batch_shape`) are
		    provided.
		  ValueError: If `shape`, `tensor` and `type_spec` are None.
		  ValueError: If arguments besides `type_spec` are non-None while `type_spec`
		              is passed.
		  ValueError: if any unrecognized parameters are provided.
	**/
	static public function Input(?shape:Dynamic, ?batch_size:Dynamic, ?name:Dynamic, ?dtype:Dynamic, ?sparse:Dynamic, ?tensor:Dynamic, ?ragged:Dynamic, ?type_spec:Dynamic, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
	static public var __builtins__ : Dynamic;
	static public var __cached__ : Dynamic;
	static public var __doc__ : Dynamic;
	static public var __file__ : Dynamic;
	static public var __loader__ : Dynamic;
	static public var __name__ : Dynamic;
	static public var __package__ : Dynamic;
	static public var __spec__ : Dynamic;
	/**
		Clone a functional `Model` instance.
		
		Model cloning is similar to calling a model on new inputs,
		except that it creates new layers (and thus new weights) instead
		of sharing the weights of the existing layers.
		
		Input layers are always cloned.
		
		Args:
		    model: Instance of `Model`.
		    input_tensors: optional list of input tensors
		        to build the model upon. If not provided,
		        placeholders will be created.
		    layer_fn: callable to be applied on non-input layers in the model. By
		        default it clones the layer. Another example is to preserve the layer
		        to share the weights. This is required when we create a per-replica
		        copy of the model with distribution strategy; we want the weights to
		        be shared but still feed inputs separately so we create new input
		        layers.
		
		Returns:
		    An instance of `Model` reproducing the behavior
		    of the original model, on top of new inputs tensors,
		    using newly instantiated weights.
		
		Raises:
		    ValueError: in case of invalid `model` argument value or `layer_fn`
		    argument value.
	**/
	static public function _clone_functional_model(model:Dynamic, ?input_tensors:Dynamic, ?layer_fn:Dynamic):Dynamic;
	static public function _clone_layer(layer:Dynamic):Dynamic;
	/**
		Clones all layers, and returns the model config without serializing layers.
		
		This function ensures that only the node graph is retrieved when getting the
		model config. The `layer_fn` used to clone layers might not rely on
		`layer.get_config()`, so some custom layers do not define `get_config`.
		Trying to retrieve the config results in errors.
		
		Args:
		  model: A Functional model.
		  input_layers: Dictionary mapping input layers in `model` to new input layers
		  layer_fn: Function used to clone all non-input layers.
		
		Returns:
		  Model config object, and a dictionary of newly created layers.
	**/
	static public function _clone_layers_and_model_config(model:Dynamic, input_layers:Dynamic, layer_fn:Dynamic):Dynamic;
	/**
		Clone a `Sequential` model instance.
		
		Model cloning is similar to calling a model on new inputs,
		except that it creates new layers (and thus new weights) instead
		of sharing the weights of the existing layers.
		
		Args:
		    model: Instance of `Sequential`.
		    input_tensors: optional list of input tensors
		        to build the model upon. If not provided,
		        placeholders will be created.
		    layer_fn: callable to be applied on non-input layers in the model. By
		        default it clones the layer. Another example is to preserve the layer
		        to share the weights. This is required when we create a per-replica
		        copy of the model with distribution strategy; we want the weights to
		        be shared but still feed inputs separately so we create new input
		        layers.
		
		Returns:
		    An instance of `Sequential` reproducing the behavior
		    of the original model, on top of new inputs tensors,
		    using newly instantiated weights.
		
		Raises:
		    ValueError: in case of invalid `model` argument value or `layer_fn`
		    argument value.
	**/
	static public function _clone_sequential_model(model:Dynamic, ?input_tensors:Dynamic, ?layer_fn:Dynamic):Dynamic;
	/**
		Substitute for model cloning that works for subclassed models.
		
		Subclassed models cannot be cloned because their topology is not serializable.
		To "instantiate" an identical model in a new TF graph, we reuse the original
		model object, but we clear its state.
		
		After calling this function on a model instance, you can use the model
		instance as if it were a model clone (in particular you can use it in a new
		graph).
		
		This method clears the state of the input model. It is thus destructive.
		However the original state can be restored fully by calling
		`_in_place_subclassed_model_state_restoration`.
		
		Args:
		  model: Instance of a Keras model created via subclassing.
		
		Raises:
		  ValueError: In case the model uses a subclassed model as inner layer.
	**/
	static public function _in_place_subclassed_model_reset(model:Dynamic):Dynamic;
	/**
		Inserts ancillary layers into the model with the proper order.
	**/
	static public function _insert_ancillary_layers(model:Dynamic, ancillary_layers:Dynamic, metrics_names:Dynamic, new_nodes:Dynamic):Dynamic;
	/**
		Uses the layers in `layer_map` to make new nodes based on `nodes_by_depth`.
		
		Args:
		  nodes_by_depth: Provides structure information to create new nodes.
		  layer_fn: Function to clone layers.
		  layer_map: Map from layers in `model` to new layers.
		  tensor_map: Map from tensors in `model` to newly compute tensors.
		
		Returns:
		  A set of new nodes. `layer_map` and `tensor_map` are updated.
	**/
	static public function _make_new_nodes(nodes_by_depth:Dynamic, layer_fn:Dynamic, layer_map:Dynamic, tensor_map:Dynamic):Dynamic;
	/**
		Removes and returns any ancillary layers from `layers` based on `model`.
		
		Ancillary layers are part of the model topology but not used to compute the
		model outputs, e.g., layers from `add_loss` and `add_metric`.
		
		Args:
		  model: A Keras Model.
		  layer_map: A map to from layers in the `model` to those in `layers`.
		  layers: A list of all layers.
		
		Returns:
		  Two lists of layers: (1) `layers` with the ancillary layers removed, and (2)
		  the ancillary layers.
	**/
	static public function _remove_ancillary_layers(model:Dynamic, layer_map:Dynamic, layers:Dynamic):Dynamic;
	/**
		Reset state trackers for model.
		
		Note that we do not actually zero out attributes such as optimizer,
		but instead rely on the expectation that all of the attrs will be
		over-written on calling build/compile/etc. This is somewhat fragile,
		insofar as we check elsewhere for the presence of these attributes as
		evidence of having been built/compiled/etc. Pending a better way to do this,
		we reset key attributes here to allow building and compiling.
		
		Args:
		  model: the model that is being reset
	**/
	static public function _reset_build_compile_trackers(model:Dynamic):Dynamic;
	/**
		Clone a `Model` and build/compile it with the same settings used before.
		
		This function can be run in the same graph or in a separate graph from the
		model. When using a separate graph, `in_place_reset` must be `False`.
		
		Note that, currently, the clone produced from this function may not work with
		TPU DistributionStrategy. Try at your own risk.
		
		Args:
		  model: `tf.keras.Model` object. Can be Functional, Sequential, or
		    sub-classed.
		  input_tensors: Optional list or dictionary of input tensors to build the
		    model upon. If not provided, placeholders will be created.
		  target_tensors: Optional list of target tensors for compiling the model. If
		    not provided, placeholders will be created.
		  custom_objects: Optional dictionary mapping string names to custom classes
		    or functions.
		  compile_clone: Boolean, whether to compile model clone (default `True`).
		  in_place_reset: Boolean, whether to reset the model in place. Only used if
		    the model is a subclassed model. In the case of a subclassed model,
		    this argument must be set to `True` (default `False`). To restore the
		    original model, use the function
		    `in_place_subclassed_model_state_restoration(model)`.
		  optimizer_iterations: An iterations variable that will be incremented by the
		    optimizer if the clone is compiled. This argument is used when a Keras
		    model is cloned into an Estimator model function, because Estimators
		    create their own global step variable.
		  optimizer_config: Optimizer config dictionary or list of dictionary
		    returned from `get_config()`. This argument should be defined if
		    `clone_and_build_model` is called in a different graph or session from
		    the original model, and the optimizer is an instance of `OptimizerV2`.
		
		Returns:
		  Clone of the model.
		
		Raises:
		  ValueError: Cloning fails in the following cases
		    - cloning a subclassed model with `in_place_reset` set to False.
		    - compiling the clone when the original model has not been compiled.
	**/
	static public function clone_and_build_model(model:Dynamic, ?input_tensors:Dynamic, ?target_tensors:Dynamic, ?custom_objects:Dynamic, ?compile_clone:Dynamic, ?in_place_reset:Dynamic, ?optimizer_iterations:Dynamic, ?optimizer_config:Dynamic):Dynamic;
	/**
		Clone a Functional or Sequential `Model` instance.
		
		Model cloning is similar to calling a model on new inputs,
		except that it creates new layers (and thus new weights) instead
		of sharing the weights of the existing layers.
		
		Note that
		`clone_model` will not preserve the uniqueness of shared objects within the
		model (e.g. a single variable attached to two distinct layers will be
		restored as two separate variables).
		
		Args:
		    model: Instance of `Model`
		        (could be a Functional model or a Sequential model).
		    input_tensors: optional list of input tensors or InputLayer objects
		        to build the model upon. If not provided,
		        new `Input` objects will be created.
		    clone_function: Callable to be used to clone each layer in the target
		        model (except `InputLayer` instances). It takes as argument the layer
		        instance to be cloned, and returns the corresponding layer instance to
		        be used in the model copy. If unspecified, this callable defaults to
		        the following serialization/deserialization function:
		        `lambda layer: layer.__class__.from_config(layer.get_config())`.
		        By passing a custom callable, you can customize your copy of the
		        model, e.g. by wrapping certain layers of interest (you might want to
		        replace all `LSTM` instances with equivalent
		        `Bidirectional(LSTM(...))` instances, for example).
		
		Returns:
		  An instance of `Model` reproducing the behavior
		  of the original model, on top of new inputs tensors,
		  using newly instantiated weights. The cloned model may behave
		  differently from the original model if a custom `clone_function`
		  modifies the layer.
		
		Example:
		
		```python
		# Create a test Sequential model.
		model = keras.Sequential([
		    keras.Input(shape=(728,)),
		    keras.layers.Dense(32, activation='relu'),
		    keras.layers.Dense(1, activation='sigmoid'),
		])
		# Create a copy of the test model (with freshly initialized weights).
		new_model = clone_model(model)
		```
		
		Note that subclassed models cannot be cloned, since their internal
		layer structure is not known. To achieve equivalent functionality
		as `clone_model` in the case of a subclassed model, simply make sure
		that the model class implements `get_config()`
		(and optionally `from_config()`), and call:
		
		```python
		new_model = model.__class__.from_config(model.get_config())
		```
	**/
	static public function clone_model(model:Dynamic, ?input_tensors:Dynamic, ?clone_function:Dynamic):Dynamic;
	/**
		Restores the original state of a model after it was "reset".
		
		This undoes this action of `_in_place_subclassed_model_reset`, which is called
		in `clone_and_build_model` if `in_place_reset` is set to True.
		
		Args:
		  model: Instance of a Keras model created via subclassing, on which
		    `_in_place_subclassed_model_reset` was previously called.
	**/
	static public function in_place_subclassed_model_state_restoration(model:Dynamic):Dynamic;
	static public function keras_export(?args:python.VarArgs<Dynamic>, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
	/**
		Loads a model saved via `model.save()`.
		
		Usage:
		
		>>> model = tf.keras.Sequential([
		...     tf.keras.layers.Dense(5, input_shape=(3,)),
		...     tf.keras.layers.Softmax()])
		>>> model.save('/tmp/model')
		>>> loaded_model = tf.keras.models.load_model('/tmp/model')
		>>> x = tf.random.uniform((10, 3))
		>>> assert np.allclose(model.predict(x), loaded_model.predict(x))
		
		Note that the model weights may have different scoped names after being
		loaded. Scoped names include the model/layer names, such as
		`"dense_1/kernel:0"`. It is recommended that you use the layer properties to
		access specific variables, e.g. `model.get_layer("dense_1").kernel`.
		
		Args:
		    filepath: One of the following:
		        - String or `pathlib.Path` object, path to the saved model
		        - `h5py.File` object from which to load the model
		    custom_objects: Optional dictionary mapping names
		        (strings) to custom classes or functions to be
		        considered during deserialization.
		    compile: Boolean, whether to compile the model
		        after loading.
		    options: Optional `tf.saved_model.LoadOptions` object that specifies
		      options for loading from SavedModel.
		
		Returns:
		    A Keras model instance. If the original model was compiled, and saved with
		    the optimizer, then the returned model will be compiled. Otherwise, the
		    model will be left uncompiled. In the case that an uncompiled model is
		    returned, a warning is displayed if the `compile` argument is set to
		    `True`.
		
		Raises:
		    ImportError: if loading from an hdf5 file and h5py is not available.
		    IOError: In case of an invalid savefile.
	**/
	static public function load_model(filepath:Dynamic, ?custom_objects:Dynamic, ?compile:Dynamic, ?options:Dynamic):Dynamic;
	/**
		Instantiates a Keras model from its config.
		
		Usage:
		```
		# for a Functional API model
		tf.keras.Model().from_config(model.get_config())
		
		# for a Sequential model
		tf.keras.Sequential().from_config(model.get_config())
		```
		
		Args:
		    config: Configuration dictionary.
		    custom_objects: Optional dictionary mapping names
		        (strings) to custom classes or functions to be
		        considered during deserialization.
		
		Returns:
		    A Keras model instance (uncompiled).
		
		Raises:
		    TypeError: if `config` is not a dictionary.
	**/
	static public function model_from_config(config:Dynamic, ?custom_objects:Dynamic):Dynamic;
	/**
		Parses a JSON model configuration string and returns a model instance.
		
		Usage:
		
		>>> model = tf.keras.Sequential([
		...     tf.keras.layers.Dense(5, input_shape=(3,)),
		...     tf.keras.layers.Softmax()])
		>>> config = model.to_json()
		>>> loaded_model = tf.keras.models.model_from_json(config)
		
		Args:
		    json_string: JSON string encoding a model configuration.
		    custom_objects: Optional dictionary mapping names
		        (strings) to custom classes or functions to be
		        considered during deserialization.
		
		Returns:
		    A Keras model instance (uncompiled).
	**/
	static public function model_from_json(json_string:Dynamic, ?custom_objects:Dynamic):Dynamic;
	/**
		Parses a yaml model configuration file and returns a model instance.
		
		Note: Since TF 2.6, this method is no longer supported and will raise a
		RuntimeError.
		
		Args:
		    yaml_string: YAML string or open file encoding a model configuration.
		    custom_objects: Optional dictionary mapping names
		        (strings) to custom classes or functions to be
		        considered during deserialization.
		
		Returns:
		    A Keras model instance (uncompiled).
		
		Raises:
		    RuntimeError: announces that the method poses a security risk
	**/
	static public function model_from_yaml(yaml_string:Dynamic, ?custom_objects:Dynamic):Dynamic;
	/**
		Saves a model as a TensorFlow SavedModel or HDF5 file.
		
		See the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)
		for details.
		
		Usage:
		
		>>> model = tf.keras.Sequential([
		...     tf.keras.layers.Dense(5, input_shape=(3,)),
		...     tf.keras.layers.Softmax()])
		>>> model.save('/tmp/model')
		>>> loaded_model = tf.keras.models.load_model('/tmp/model')
		>>> x = tf.random.uniform((10, 3))
		>>> assert np.allclose(model.predict(x), loaded_model.predict(x))
		
		The SavedModel and HDF5 file contains:
		
		- the model's configuration (topology)
		- the model's weights
		- the model's optimizer's state (if any)
		
		Thus models can be reinstantiated in the exact same state, without any of the
		code used for model definition or training.
		
		Note that the model weights may have different scoped names after being
		loaded. Scoped names include the model/layer names, such as
		`"dense_1/kernel:0"`. It is recommended that you use the layer properties to
		access specific variables, e.g. `model.get_layer("dense_1").kernel`.
		
		__SavedModel serialization format__
		
		Keras SavedModel uses `tf.saved_model.save` to save the model and all
		trackable objects attached to the model (e.g. layers and variables). The model
		config, weights, and optimizer are saved in the SavedModel. Additionally, for
		every Keras layer attached to the model, the SavedModel stores:
		
		  * the config and metadata -- e.g. name, dtype, trainable status
		  * traced call and loss functions, which are stored as TensorFlow subgraphs.
		
		The traced functions allow the SavedModel format to save and load custom
		layers without the original class definition.
		
		You can choose to not save the traced functions by disabling the `save_traces`
		option. This will decrease the time it takes to save the model and the
		amount of disk space occupied by the output SavedModel. If you enable this
		option, then you _must_ provide all custom class definitions when loading
		the model. See the `custom_objects` argument in `tf.keras.models.load_model`.
		
		Args:
		    model: Keras model instance to be saved.
		    filepath: One of the following:
		      - String or `pathlib.Path` object, path where to save the model
		      - `h5py.File` object where to save the model
		    overwrite: Whether we should overwrite any existing model at the target
		      location, or instead ask the user with a manual prompt.
		    include_optimizer: If True, save optimizer's state together.
		    save_format: Either 'tf' or 'h5', indicating whether to save the model
		      to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5'
		      in TF 1.X.
		    signatures: Signatures to save with the SavedModel. Applicable to the 'tf'
		      format only. Please see the `signatures` argument in
		      `tf.saved_model.save` for details.
		    options: (only applies to SavedModel format) `tf.saved_model.SaveOptions`
		      object that specifies options for saving to SavedModel.
		    save_traces: (only applies to SavedModel format) When enabled, the
		      SavedModel will store the function traces for each layer. This
		      can be disabled, so that only the configs of each layer are stored.
		      Defaults to `True`. Disabling this will decrease serialization time and
		      reduce file size, but it requires that all custom layers/models
		      implement a `get_config()` method.
		
		Raises:
		    ImportError: If save format is hdf5, and h5py is not available.
	**/
	static public function save_model(model:Dynamic, filepath:Dynamic, ?overwrite:Dynamic, ?include_optimizer:Dynamic, ?save_format:Dynamic, ?signatures:Dynamic, ?options:Dynamic, ?save_traces:Dynamic):Dynamic;
	static public function share_weights(layer:Dynamic):Dynamic;
}