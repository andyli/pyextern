/* This file is generated, do not edit! */
package tensorflow.python.keras.layers.recurrent_v2;
@:pythonImport("tensorflow.python.keras.layers.recurrent_v2") extern class Recurrent_v2_Module {
	static public var _CPU_DEVICE_NAME : Dynamic;
	static public var _CUDNN_AVAILABLE_MSG : Dynamic;
	static public var _CUDNN_NOT_AVAILABLE_MSG : Dynamic;
	static public var _FUNCTION_API_NAME_ATTRIBUTE : Dynamic;
	static public var _FUNCTION_DEVICE_ATTRIBUTE : Dynamic;
	static public var _GPU_DEVICE_NAME : Dynamic;
	static public var _RUNTIME_CPU : Dynamic;
	static public var _RUNTIME_GPU : Dynamic;
	static public var _RUNTIME_UNKNOWN : Dynamic;
	static public var __builtins__ : Dynamic;
	static public var __cached__ : Dynamic;
	static public var __doc__ : Dynamic;
	static public var __file__ : Dynamic;
	static public var __loader__ : Dynamic;
	static public var __name__ : Dynamic;
	static public var __package__ : Dynamic;
	static public var __spec__ : Dynamic;
	/**
		Utility function convert variable to CuDNN compatible parameter.
		
		Note that Keras weights for kernels are different from the CuDNN format. Eg.:
		
		```
		  Keras                 CuDNN
		  [[0, 1, 2],  <--->  [[0, 2, 4],
		   [3, 4, 5]]          [1, 3, 5]]
		```
		
		If the input weights need to be in a unified format, then set
		`transpose_weights=True` to convert the weights.
		
		Args:
		  weights: list of weights for the individual kernels and recurrent kernels.
		  biases: list of biases for individual gate.
		  shape: the shape for the converted variables that will be feed to CuDNN.
		  transpose_weights: boolean, whether to transpose the weights.
		
		Returns:
		  The converted weights that can be feed to CuDNN ops as param.
	**/
	static public function _canonical_to_params(weights:Dynamic, biases:Dynamic, shape:Dynamic, ?transpose_weights:Dynamic):Dynamic;
	/**
		Register a specialization of a `Function` into the graph.
		
		This won't actually call the function with the inputs, and only put the
		function definition into graph. Register function with different input param
		will result into multiple version of functions registered in graph.
		
		Args:
		  func: the `Function` instance that generated by a @defun
		  *args: input arguments for the Python function.
		  **kwargs: input keyword arguments for the Python function.
		
		Returns:
		  a `ConcreteFunction` object specialized to inputs and execution context.
		
		Raises:
		  ValueError: When the input function is not a defun wrapped python function.
	**/
	static public function _function_register(func:Dynamic, ?args:python.VarArgs<Dynamic>, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
	static public function _generate_defun_backend(unique_api_name:Dynamic, preferred_device:Dynamic, func:Dynamic, supportive_attributes:Dynamic):Dynamic;
	/**
		Parse the current context and return the device type, eg CPU/GPU.
	**/
	static public function _get_context_device_type():Dynamic;
	/**
		Read the value of a variable if it is variable.
	**/
	static public function _read_variable_value(v:Dynamic):Dynamic;
	static public function _runtime(runtime_name:Dynamic):Dynamic;
	static public function _use_new_code():Dynamic;
	/**
		Calculate the sequence length tensor (1-D) based on the masking tensor.
		
		The masking tensor is a 2D boolean tensor with shape [batch, timestep]. For
		any timestep that should be masked, the corresponding field will be False.
		Consider the following example:
		  a = [[True, True, False, False],
		       [True, True, True, False]]
		It is a (2, 4) tensor, and the corresponding sequence length result should be
		1D tensor with value [2, 3]. Note that the masking tensor must be right
		padded that could be checked by, e.g., `is_sequence_right_padded()`.
		
		Args:
		  mask: Boolean tensor with shape [batch, timestep] or [timestep, batch] if
		    time_major=True.
		  time_major: Boolean, which indicates whether the mask is time major or batch
		    major.
		Returns:
		  sequence_length: 1D int32 tensor.
	**/
	static public function calculate_sequence_by_mask(mask:Dynamic, time_major:Dynamic):Dynamic;
	/**
		Get the device name for the current thread.
		
		Returns:
		  The device name for the current thread.
	**/
	static public function get_device_name():Dynamic;
	/**
		GRU with CuDNN implementation which is only available for GPU.
	**/
	static public function gpu_gru(inputs:Dynamic, init_h:Dynamic, kernel:Dynamic, recurrent_kernel:Dynamic, bias:Dynamic, mask:Dynamic, time_major:Dynamic, go_backwards:Dynamic, sequence_lengths:Dynamic):Dynamic;
	/**
		LSTM with either CuDNN or ROCm implementation which is only available for GPU.
		
		Note that currently only right padded data is supported, or the result will be
		polluted by the unmasked data which should be filtered.
		
		Args:
		  inputs: Input tensor of LSTM layer.
		  init_h: Initial state tensor for the cell output.
		  init_c: Initial state tensor for the cell hidden state.
		  kernel: Weights for cell kernel.
		  recurrent_kernel: Weights for cell recurrent kernel.
		  bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
		    is used in this case.
		  mask: Boolean tensor for mask out the steps within sequence.
		    An individual `True` entry indicates that the corresponding timestep
		    should be utilized, while a `False` entry indicates that the corresponding
		    timestep should be ignored.
		  time_major: Boolean, whether the inputs are in the format of [time, batch,
		    feature] or [batch, time, feature].
		  go_backwards: Boolean (default False). If True, process the input sequence
		    backwards and return the reversed sequence.
		  sequence_lengths: The lengths of all sequences coming from a variable length
		    input, such as ragged tensors. If the input has a fixed timestep size,
		    this should be None.
		
		Returns:
		  last_output: Output tensor for the last timestep, which has shape
		    [batch, units].
		  outputs: Output tensor for all timesteps, which has shape
		    [batch, time, units].
		  state_0: The cell output, which has same shape as init_h.
		  state_1: The cell hidden state, which has same shape as init_c.
		  runtime: Constant string tensor which indicate real runtime hardware. This
		    value is for testing purpose and should not be used by user.
	**/
	static public function gpu_lstm(inputs:Dynamic, init_h:Dynamic, init_c:Dynamic, kernel:Dynamic, recurrent_kernel:Dynamic, bias:Dynamic, mask:Dynamic, time_major:Dynamic, go_backwards:Dynamic, sequence_lengths:Dynamic):Dynamic;
	/**
		Call the GRU with optimized backend kernel selection.
		
		Under the hood, this function will create two TF function, one with the most
		generic kernel and can run on all device condition, and the second one with
		CuDNN specific kernel, which can only run on GPU.
		
		The first function will be called with normal_lstm_params, while the second
		function is not called, but only registered in the graph. The Grappler will
		do the proper graph rewrite and swap the optimized TF function based on the
		device placement.
		
		Args:
		  inputs: Input tensor of GRU layer.
		  init_h: Initial state tensor for the cell output.
		  kernel: Weights for cell kernel.
		  recurrent_kernel: Weights for cell recurrent kernel.
		  bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
		    is used in this case.
		  mask: Boolean tensor for mask out the steps within sequence.
		    An individual `True` entry indicates that the corresponding timestep
		    should be utilized, while a `False` entry indicates that the corresponding
		    timestep should be ignored.
		  time_major: Boolean, whether the inputs are in the format of
		    [time, batch, feature] or [batch, time, feature].
		  go_backwards: Boolean (default False). If True, process the input sequence
		    backwards and return the reversed sequence.
		  sequence_lengths: The lengths of all sequences coming from a variable length
		    input, such as ragged tensors. If the input has a fixed timestep size,
		    this should be None.
		  zero_output_for_mask: Boolean, whether to output zero for masked timestep.
		
		Returns:
		  List of output tensors, same as standard_gru.
	**/
	static public function gru_with_backend_selection(inputs:Dynamic, init_h:Dynamic, kernel:Dynamic, recurrent_kernel:Dynamic, bias:Dynamic, mask:Dynamic, time_major:Dynamic, go_backwards:Dynamic, sequence_lengths:Dynamic, zero_output_for_mask:Dynamic):Dynamic;
	static public function has_fully_masked_sequence(mask:Dynamic):Dynamic;
	static public function is_cudnn_supported_inputs(mask:Dynamic, time_major:Dynamic):Dynamic;
	/**
		Check the mask tensor and see if it right padded.
		
		For CuDNN kernel, it uses the sequence length param to skip the tailing
		timestep. If the data is left padded, or not a strict right padding (has
		masked value in the middle of the sequence), then CuDNN kernel won't be work
		properly in those cases.
		
		Left padded data: [[False, False, True, True, True]].
		Right padded data: [[True, True, True, False, False]].
		Mixture of mask/unmasked data: [[True, False, True, False, False]].
		
		Note that for the mixed data example above, the actually data RNN should see
		are those 2 Trues (index 0 and 2), the index 1 False should be ignored and not
		pollute the internal states.
		
		Args:
		  mask: the Boolean tensor with shape [batch, timestep]
		
		Returns:
		  boolean scalar tensor, whether the mask is strictly right padded.
	**/
	static public function is_sequence_right_padded(mask:Dynamic):Dynamic;
	static public function keras_export(?args:python.VarArgs<Dynamic>, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
	/**
		Call the LSTM with optimized backend kernel selection.
		
		Under the hood, this function will create two TF function, one with the most
		generic kernel and can run on all device condition, and the second one with
		CuDNN specific kernel, which can only run on GPU.
		
		The first function will be called with normal_lstm_params, while the second
		function is not called, but only registered in the graph. The Grappler will
		do the proper graph rewrite and swap the optimized TF function based on the
		device placement.
		
		Args:
		  inputs: Input tensor of LSTM layer.
		  init_h: Initial state tensor for the cell output.
		  init_c: Initial state tensor for the cell hidden state.
		  kernel: Weights for cell kernel.
		  recurrent_kernel: Weights for cell recurrent kernel.
		  bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
		    is used in this case.
		  mask: Boolean tensor for mask out the steps within sequence.
		    An individual `True` entry indicates that the corresponding timestep
		    should be utilized, while a `False` entry indicates that the corresponding
		    timestep should be ignored.
		  time_major: Boolean, whether the inputs are in the format of
		    [time, batch, feature] or [batch, time, feature].
		  go_backwards: Boolean (default False). If True, process the input sequence
		    backwards and return the reversed sequence.
		  sequence_lengths: The lengths of all sequences coming from a variable length
		    input, such as ragged tensors. If the input has a fixed timestep size,
		    this should be None.
		  zero_output_for_mask: Boolean, whether to output zero for masked timestep.
		
		Returns:
		  List of output tensors, same as standard_lstm.
	**/
	static public function lstm_with_backend_selection(inputs:Dynamic, init_h:Dynamic, init_c:Dynamic, kernel:Dynamic, recurrent_kernel:Dynamic, bias:Dynamic, mask:Dynamic, time_major:Dynamic, go_backwards:Dynamic, sequence_lengths:Dynamic, zero_output_for_mask:Dynamic):Dynamic;
	/**
		GRU with standard kernel implementation.
		
		This implementation can be run on all types of hardware.
		
		This implementation lifts out all the layer weights and make them function
		parameters. It has same number of tensor input params as the CuDNN
		counterpart. The RNN step logic has been simplified, eg dropout and mask is
		removed since CuDNN implementation does not support that.
		
		Args:
		  inputs: Input tensor of GRU layer.
		  init_h: Initial state tensor for the cell output.
		  kernel: Weights for cell kernel.
		  recurrent_kernel: Weights for cell recurrent kernel.
		  bias: Weights for cell kernel bias and recurrent bias. The bias contains the
		    combined input_bias and recurrent_bias.
		  mask: Binary tensor of shape `(samples, timesteps)` indicating whether
		    a given timestep should be masked. An individual `True` entry indicates
		    that the corresponding timestep should be utilized, while a `False` entry
		    indicates that the corresponding timestep should be ignored.
		  time_major: Boolean, whether the inputs are in the format of
		    [time, batch, feature] or [batch, time, feature].
		  go_backwards: Boolean (default False). If True, process the input sequence
		    backwards and return the reversed sequence.
		  sequence_lengths: The lengths of all sequences coming from a variable length
		    input, such as ragged tensors. If the input has a fixed timestep size,
		    this should be None.
		  zero_output_for_mask: Boolean, whether to output zero for masked timestep.
		
		Returns:
		  last_output: output tensor for the last timestep, which has shape
		    [batch, units].
		  outputs: output tensor for all timesteps, which has shape
		    [batch, time, units].
		  state_0: the cell output, which has same shape as init_h.
		  runtime: constant string tensor which indicate real runtime hardware. This
		    value is for testing purpose and should be used by user.
	**/
	static public function standard_gru(inputs:Dynamic, init_h:Dynamic, kernel:Dynamic, recurrent_kernel:Dynamic, bias:Dynamic, mask:Dynamic, time_major:Dynamic, go_backwards:Dynamic, sequence_lengths:Dynamic, zero_output_for_mask:Dynamic):Dynamic;
	/**
		LSTM with standard kernel implementation.
		
		This implementation can be run on all types for hardware.
		
		This implementation lifts out all the layer weights and make them function
		parameters. It has same number of tensor input params as the CuDNN
		counterpart. The RNN step logic has been simplified, eg dropout and mask is
		removed since CuDNN implementation does not support that.
		
		Note that the first half of the bias tensor should be ignored by this impl.
		The CuDNN impl need an extra set of input gate bias. In order to make the both
		function take same shape of parameter, that extra set of bias is also feed
		here.
		
		Args:
		  inputs: input tensor of LSTM layer.
		  init_h: initial state tensor for the cell output.
		  init_c: initial state tensor for the cell hidden state.
		  kernel: weights for cell kernel.
		  recurrent_kernel: weights for cell recurrent kernel.
		  bias: weights for cell kernel bias and recurrent bias. Only recurrent bias
		    is used in this case.
		  mask: Boolean tensor for mask out the steps within sequence.
		    An individual `True` entry indicates that the corresponding timestep
		    should be utilized, while a `False` entry indicates that the corresponding
		    timestep should be ignored.
		  time_major: boolean, whether the inputs are in the format of
		    [time, batch, feature] or [batch, time, feature].
		  go_backwards: Boolean (default False). If True, process the input sequence
		    backwards and return the reversed sequence.
		  sequence_lengths: The lengths of all sequences coming from a variable length
		    input, such as ragged tensors. If the input has a fixed timestep size,
		    this should be None.
		  zero_output_for_mask: Boolean, whether to output zero for masked timestep.
		
		Returns:
		  last_output: output tensor for the last timestep, which has shape
		    [batch, units].
		  outputs: output tensor for all timesteps, which has shape
		    [batch, time, units].
		  state_0: the cell output, which has same shape as init_h.
		  state_1: the cell hidden state, which has same shape as init_c.
		  runtime: constant string tensor which indicate real runtime hardware. This
		    value is for testing purpose and should be used by user.
	**/
	static public function standard_lstm(inputs:Dynamic, init_h:Dynamic, init_c:Dynamic, kernel:Dynamic, recurrent_kernel:Dynamic, bias:Dynamic, mask:Dynamic, time_major:Dynamic, go_backwards:Dynamic, sequence_lengths:Dynamic, zero_output_for_mask:Dynamic):Dynamic;
}