/* This file is generated, do not edit! */
package tensorflow.python.distribute.coordinator.cluster_coordinator;
@:pythonImport("tensorflow.python.distribute.coordinator.cluster_coordinator", "ClusterCoordinator") extern class ClusterCoordinator {
	public function __class__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	public function __del__():Dynamic;
	/**
		Implement delattr(self, name).
	**/
	public function __delattr__(name:Dynamic):Dynamic;
	static public var __dict__ : Dynamic;
	/**
		Default dir() implementation.
	**/
	public function __dir__():Dynamic;
	static public var __doc__ : Dynamic;
	/**
		Return self==value.
	**/
	public function __eq__(value:Dynamic):Dynamic;
	/**
		Default object formatter.
	**/
	public function __format__(format_spec:Dynamic):Dynamic;
	/**
		Return self>=value.
	**/
	public function __ge__(value:Dynamic):Dynamic;
	/**
		Return getattr(self, name).
	**/
	public function __getattribute__(name:Dynamic):Dynamic;
	/**
		Return self>value.
	**/
	public function __gt__(value:Dynamic):Dynamic;
	/**
		Return hash(self).
	**/
	public function __hash__():Dynamic;
	/**
		Initialization of a `ClusterCoordinator` instance.
		
		Args:
		  strategy: a supported `tf.distribute.Strategy` object. Currently, only
		    `tf.distribute.experimental.ParameterServerStrategy` is supported.
		
		Raises:
		  ValueError: if the strategy being used is not supported.
	**/
	@:native("__init__")
	public function ___init__(strategy:Dynamic):Dynamic;
	/**
		Initialization of a `ClusterCoordinator` instance.
		
		Args:
		  strategy: a supported `tf.distribute.Strategy` object. Currently, only
		    `tf.distribute.experimental.ParameterServerStrategy` is supported.
		
		Raises:
		  ValueError: if the strategy being used is not supported.
	**/
	public function new(strategy:Dynamic):Void;
	/**
		This method is called when a class is subclassed.
		
		The default implementation does nothing. It may be
		overridden to extend subclasses.
	**/
	public function __init_subclass__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Return self<=value.
	**/
	public function __le__(value:Dynamic):Dynamic;
	/**
		Return self<value.
	**/
	public function __lt__(value:Dynamic):Dynamic;
	static public var __module__ : Dynamic;
	/**
		Return self!=value.
	**/
	public function __ne__(value:Dynamic):Dynamic;
	/**
		Create and return a new object.  See help(type) for accurate signature.
	**/
	static public function __new__(cls:Dynamic, strategy:Dynamic):Dynamic;
	/**
		Helper for pickle.
	**/
	public function __reduce__():Dynamic;
	/**
		Helper for pickle.
	**/
	public function __reduce_ex__(protocol:Dynamic):Dynamic;
	/**
		Return repr(self).
	**/
	public function __repr__():Dynamic;
	/**
		Implement setattr(self, name, value).
	**/
	public function __setattr__(name:Dynamic, value:Dynamic):Dynamic;
	/**
		Size of object in memory, in bytes.
	**/
	public function __sizeof__():Dynamic;
	/**
		Return str(self).
	**/
	public function __str__():Dynamic;
	/**
		Abstract classes can override this to customize issubclass().
		
		This is invoked early on by abc.ABCMeta.__subclasscheck__().
		It should return True, False or NotImplemented.  If it returns
		NotImplemented, the normal algorithm is used.  Otherwise, it
		overrides the normal algorithm (and the outcome is cached).
	**/
	public function __subclasshook__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		list of weak references to the object (if defined)
	**/
	public var __weakref__ : Dynamic;
	/**
		Synchronously create resources on the workers.
		
		The resources are represented by
		`tf.distribute.experimental.coordinator.RemoteValue`s.
		
		Args:
		  fn: The function to be dispatched to all workers for execution
		    asynchronously.
		  args: Positional arguments for `fn`.
		  kwargs: Keyword arguments for `fn`.
		
		Returns:
		  A `tf.distribute.experimental.coordinator.PerWorkerValues` object, which
		  wraps a tuple of `tf.distribute.experimental.coordinator.RemoteValue`
		  objects.
	**/
	public function _create_per_worker_resources(fn:Dynamic, ?args:Dynamic, ?kwargs:Dynamic):Dynamic;
	static public var _tf_api_names : Dynamic;
	static public var _tf_api_names_v1 : Dynamic;
	/**
		Create dataset on workers by calling `dataset_fn` on worker devices.
		
		This creates the given dataset generated by dataset_fn on workers
		and returns an object that represents the collection of those individual
		datasets. Calling `iter` on such collection of datasets returns a
		`tf.distribute.experimental.coordinator.PerWorkerValues`, which is a
		collection of iterators, where the iterators have been placed on respective
		workers.
		
		Calling `next` on a `PerWorkerValues` of iterator is unsupported. The
		iterator is meant to be passed as an argument into
		`tf.distribute.experimental.coordinator.ClusterCoordinator.schedule`. When
		the scheduled function is about to be executed by a worker, the
		function will receive the individual iterator that corresponds to the
		worker. The `next` method can be called on an iterator inside a
		scheduled function when the iterator is an input of the function.
		
		Currently the `schedule` method assumes workers are all the same and thus
		assumes the datasets on different workers are the same, except they may be
		shuffled differently if they contain a `dataset.shuffle` operation and a
		random seed is not set. Because of this, we also recommend the datasets to
		be repeated indefinitely and schedule a finite number of steps instead of
		relying on the `OutOfRangeError` from a dataset.
		
		
		Example:
		
		```python
		strategy = tf.distribute.experimental.ParameterServerStrategy(
		    cluster_resolver=...)
		coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(
		    strategy=strategy)
		
		@tf.function
		def worker_fn(iterator):
		  return next(iterator)
		
		def per_worker_dataset_fn():
		  return strategy.distribute_datasets_from_function(
		      lambda x: tf.data.Dataset.from_tensor_slices([3] * 3))
		
		per_worker_dataset = coordinator.create_per_worker_dataset(
		    per_worker_dataset_fn)
		per_worker_iter = iter(per_worker_dataset)
		remote_value = coordinator.schedule(worker_fn, args=(per_worker_iter,))
		assert remote_value.fetch() == 3
		```
		
		Args:
		  dataset_fn: The dataset function that returns a dataset. This is to be
		    executed on the workers.
		
		Returns:
		  An object that represents the collection of those individual
		  datasets. `iter` is expected to be called on this object that returns
		  a `tf.distribute.experimental.coordinator.PerWorkerValues` of the
		  iterators (that are on the workers).
	**/
	public function create_per_worker_dataset(dataset_fn:Dynamic):Dynamic;
	/**
		Returns whether all the scheduled functions have finished execution.
		
		If any previously scheduled function raises an error, `done` will fail by
		raising any one of those errors.
		
		When `done` returns True or raises, it guarantees that there is no function
		that is still being executed.
		
		Returns:
		  Whether all the scheduled functions have finished execution.
		Raises:
		  Exception: one of the exceptions caught by the coordinator by any
		    previously scheduled function since the last time an error was thrown or
		    since the beginning of the program.
	**/
	public function done():Dynamic;
	/**
		Blocking call to fetch results from the remote values.
		
		This is a wrapper around
		`tf.distribute.experimental.coordinator.RemoteValue.fetch` for a
		`RemoteValue` structure; it returns the execution results of
		`RemoteValue`s. If not ready, wait for them while blocking the caller.
		
		Example:
		```python
		strategy = ...
		coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(
		    strategy)
		
		def dataset_fn():
		  return tf.data.Dataset.from_tensor_slices([1, 1, 1])
		
		with strategy.scope():
		  v = tf.Variable(initial_value=0)
		
		@tf.function
		def worker_fn(iterator):
		  def replica_fn(x):
		    v.assign_add(x)
		    return v.read_value()
		  return strategy.run(replica_fn, args=(next(iterator),))
		
		distributed_dataset = coordinator.create_per_worker_dataset(dataset_fn)
		distributed_iterator = iter(distributed_dataset)
		result = coordinator.schedule(worker_fn, args=(distributed_iterator,))
		assert coordinator.fetch(result) == 1
		```
		
		Args:
		  val: The value to fetch the results from. If this is structure of
		    `tf.distribute.experimental.coordinator.RemoteValue`, `fetch()` will be
		    called on the individual
		    `tf.distribute.experimental.coordinator.RemoteValue` to get the result.
		
		Returns:
		  If `val` is a `tf.distribute.experimental.coordinator.RemoteValue` or a
		  structure of `tf.distribute.experimental.coordinator.RemoteValue`s,
		  return the fetched `tf.distribute.experimental.coordinator.RemoteValue`
		  values immediately if they are available, or block the call until they are
		  available, and return the fetched
		  `tf.distribute.experimental.coordinator.RemoteValue` values with the same
		  structure. If `val` is other types, return it as-is.
	**/
	public function fetch(val:Dynamic):Dynamic;
	/**
		Blocks until all the scheduled functions have finished execution.
		
		If any previously scheduled function raises an error, `join` will fail by
		raising any one of those errors, and clear the errors collected so far. If
		this happens, some of the previously scheduled functions may have not been
		executed. Users can call `fetch` on the returned
		`tf.distribute.experimental.coordinator.RemoteValue` to inspect if they have
		executed, failed, or cancelled. If some that have been cancelled need to be
		rescheduled, users should call `schedule` with the function again.
		
		When `join` returns or raises, it guarantees that there is no function that
		is still being executed.
		
		Raises:
		  Exception: one of the exceptions caught by the coordinator by any
		    previously scheduled function since the last time an error was thrown or
		    since the beginning of the program.
	**/
	public function join():Dynamic;
	/**
		Schedules `fn` to be dispatched to a worker for asynchronous execution.
		
		This method is non-blocking in that it queues the `fn` which will be
		executed later and returns a
		`tf.distribute.experimental.coordinator.RemoteValue` object immediately.
		`fetch` can be called on it to wait for the function execution to finish
		and retrieve its output from a remote worker. On the other hand, call
		`tf.distribute.experimental.coordinator.ClusterCoordinator.join` to wait for
		all scheduled functions to finish.
		
		`schedule` guarantees that `fn` will be executed on a worker at least once;
		it could be more than once if its corresponding worker fails in the middle
		of its execution. Note that since worker can fail at any point when
		executing the function, it is possible that the function is partially
		executed, but `tf.distribute.experimental.coordinator.ClusterCoordinator`
		guarantees that in those events, the function will eventually be executed on
		any worker that is available.
		
		If any previously scheduled function raises an error, `schedule` will raise
		any one of those errors, and clear the errors collected so far. What happens
		here, some of the previously scheduled functions may have not been executed.
		User can call `fetch` on the returned
		`tf.distribute.experimental.coordinator.RemoteValue` to inspect if they have
		executed, failed, or cancelled, and reschedule the corresponding function if
		needed.
		
		When `schedule` raises, it guarantees that there is no function that is
		still being executed.
		
		At this time, there is no support of worker assignment for function
		execution, or priority of the workers.
		
		`args` and `kwargs` are the arguments passed into `fn`, when `fn` is
		executed on a worker. They can be
		`tf.distribute.experimental.coordinator.PerWorkerValues` and in this case,
		the argument will be substituted with the corresponding component on the
		target worker. Arguments that are not
		`tf.distribute.experimental.coordinator.PerWorkerValues` will be passed into
		`fn` as-is. Currently, `tf.distribute.experimental.coordinator.RemoteValue`
		is not supported to be input `args` or `kwargs`.
		
		Args:
		  fn: A `tf.function`; the function to be dispatched to a worker for
		    execution asynchronously. Regular python funtion is not supported to be
		    scheduled.
		  args: Positional arguments for `fn`.
		  kwargs: Keyword arguments for `fn`.
		
		Returns:
		  A `tf.distribute.experimental.coordinator.RemoteValue` object that
		  represents the output of the function scheduled.
		
		Raises:
		  Exception: one of the exceptions caught by the coordinator from any
		    previously scheduled function, since the last time an error was thrown
		    or since the beginning of the program.
	**/
	public function schedule(fn:Dynamic, ?args:Dynamic, ?kwargs:Dynamic):Dynamic;
	/**
		Returns the `Strategy` associated with the `ClusterCoordinator`.
	**/
	public var strategy : Dynamic;
}