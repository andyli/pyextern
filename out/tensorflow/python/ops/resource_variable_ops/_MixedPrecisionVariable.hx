/* This file is generated, do not edit! */
package tensorflow.python.ops.resource_variable_ops;
@:pythonImport("tensorflow.python.ops.resource_variable_ops", "_MixedPrecisionVariable") extern class _MixedPrecisionVariable {
	/**
		Information on how to save this Variable as a slice.
		
		Provides internal support for saving variables as slices of a larger
		variable.  This API is not public and is subject to change.
		
		Available properties:
		
		* full_name
		* full_shape
		* var_offset
		* var_shape
	**/
	static public function SaveSliceInfo(?full_name:Dynamic, ?full_shape:Dynamic, ?var_offset:Dynamic, ?var_shape:Dynamic, ?save_slice_info_def:Dynamic, ?import_scope:Dynamic):Dynamic;
	/**
		Register overloads for all operators.
	**/
	static public function _OverloadAllOperators():Dynamic;
	/**
		Defer an operator overload to `ops.Tensor`.
		
		We pull the operator out of ops.Tensor dynamically to avoid ordering issues.
		
		Args:
		  operator: string. The operator name.
	**/
	static public function _OverloadOperator(_operator:Dynamic):Dynamic;
	/**
		Utility function for converting a Variable to a Tensor.
	**/
	static public function _TensorConversionFunction(v:Dynamic, ?dtype:Dynamic, ?name:Dynamic, ?as_ref:Dynamic):Dynamic;
	/**
		Computes the absolute value of a tensor.
		
		Given a tensor `x` of complex numbers, this operation returns a tensor of type
		`float32` or `float64` that is the absolute value of each element in `x`. All
		elements in `x` must be complex numbers of the form \\(a + bj\\). The
		absolute value is computed as \\( \sqrt{a^2 + b^2}\\).  For example:
		```python
		x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])
		tf.abs(x)  # [5.25594902, 6.60492229]
		```
		
		Args:
		  x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,
		    `int32`, `int64`, `complex64` or `complex128`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` or `SparseTensor` the same size and type as `x` with absolute
		    values.
		  Note, for `complex64` or `complex128` input, the returned `Tensor` will be
		    of type `float32` or `float64`, respectively.
	**/
	static public function __abs__(x:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Returns x + y element-wise.
		
		*NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __add__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		Returns the truth value of x AND y element-wise.
		
		*NOTE*: `math.logical_and` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor` of type `bool`.
		  y: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __and__(x:Dynamic, y:Dynamic):Dynamic;
	static public var __array_priority__ : Dynamic;
	public function __bool__():Dynamic;
	/**
		Metaclass to allow construction of tf.Variable to be overridden.
	**/
	public function __class__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	public function __copy__():Dynamic;
	public function __deepcopy__(memo:Dynamic):Dynamic;
	/**
		Implement delattr(self, name).
	**/
	public function __delattr__(name:Dynamic):Dynamic;
	static public var __dict__ : Dynamic;
	/**
		__dir__() -> list
		default dir() implementation
	**/
	public function __dir__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Divide two values using Python 2 semantics. Used for Tensor.__div__.
		
		Args:
		  x: `Tensor` numerator of real numeric type.
		  y: `Tensor` denominator of real numeric type.
		  name: A name for the operation (optional).
		Returns:
		  `x / y` returns the quotient of x and y.
	**/
	static public function __div__(x:Dynamic, y:Dynamic):Dynamic;
	static public var __doc__ : Dynamic;
	/**
		Return self==value.
	**/
	public function __eq__(value:Dynamic):Dynamic;
	/**
		Divides `x / y` elementwise, rounding toward the most negative integer.
		
		The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for
		floating point arguments so that the result is always an integer (though
		possibly an integer represented as floating point).  This op is generated by
		`x // y` floor division in Python 3 and in Python 2.7 with
		`from __future__ import division`.
		
		`x` and `y` must have the same type, and the result will have the same type
		as well.
		
		Args:
		  x: `Tensor` numerator of real numeric type.
		  y: `Tensor` denominator of real numeric type.
		  name: A name for the operation (optional).
		
		Returns:
		  `x / y` rounded down.
		
		Raises:
		  TypeError: If the inputs are complex.
	**/
	static public function __floordiv__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		default object formatter
	**/
	public function __format__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Returns the truth value of (x >= y) element-wise.
		
		*NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __ge__(x:Dynamic, y:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Return getattr(self, name).
	**/
	public function __getattribute__(name:Dynamic):Dynamic;
	/**
		Creates a slice helper object given a variable.
		
		This allows creating a sub-tensor from part of the current contents
		of a variable. See `tf.Tensor.__getitem__` for detailed examples
		of slicing.
		
		This function in addition also allows assignment to a sliced range.
		This is similar to `__setitem__` functionality in Python. However,
		the syntax is different so that the user can capture the assignment
		operation for grouping or passing to `sess.run()`.
		For example,
		
		```python
		import tensorflow as tf
		A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)
		with tf.Session() as sess:
		  sess.run(tf.global_variables_initializer())
		  print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]
		
		  op = A[:2,:2].assign(22. * tf.ones((2, 2)))
		  print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]
		```
		
		Note that assignments currently do not support NumPy broadcasting
		semantics.
		
		Args:
		  var: An `ops.Variable` object.
		  slice_spec: The arguments to `Tensor.__getitem__`.
		
		Returns:
		  The appropriate slice of "tensor", based on "slice_spec".
		  As an operator. The operator also has a `assign()` method
		  that can be used to generate an assignment operator.
		
		Raises:
		  ValueError: If a slice range is negative size.
		  TypeError: TypeError: If the slice indices aren't int, slice,
		    ellipsis, tf.newaxis or int32/int64 tensors.
	**/
	static public function __getitem__(_var:Dynamic, slice_spec:Dynamic):Dynamic;
	/**
		Returns the truth value of (x > y) element-wise.
		
		*NOTE*: `math.greater` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __gt__(x:Dynamic, y:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Return hash(self).
	**/
	public function __hash__():Dynamic;
	public function __iadd__(unused_other:Dynamic):Dynamic;
	public function __idiv__(unused_other:Dynamic):Dynamic;
	public function __imul__(unused_other:Dynamic):Dynamic;
	/**
		Creates a MixedPrecisionVariable.
		
		Args:
		  var: A ResourceVariable instance.
		  read_dtype: A tf.DType, the returned dtype when read, default to None.
		    Casting is performed if read_dtype is not None and differs from
		    var.dtype.
		Returns:
		  An MixedPrecisionVariable instance.
		Raises:
		  ValueError: if var is not a ResourceVariable instance, or read_dtype is
		    not a tf.DType instance.
	**/
	@:native("__init__")
	public function ___init__(_var:Dynamic, read_dtype:Dynamic):Dynamic;
	/**
		Creates a MixedPrecisionVariable.
		
		Args:
		  var: A ResourceVariable instance.
		  read_dtype: A tf.DType, the returned dtype when read, default to None.
		    Casting is performed if read_dtype is not None and differs from
		    var.dtype.
		Returns:
		  An MixedPrecisionVariable instance.
		Raises:
		  ValueError: if var is not a ResourceVariable instance, or read_dtype is
		    not a tf.DType instance.
	**/
	public function new(_var:Dynamic, read_dtype:Dynamic):Void;
	/**
		This method is called when a class is subclassed.
		
		The default implementation does nothing. It may be
		overridden to extend subclasses.
	**/
	public function __init_subclass__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	public function __int__():Dynamic;
	/**
		Returns the truth value of NOT x element-wise.
		
		Args:
		  x: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __invert__(x:Dynamic, ?name:Dynamic):Dynamic;
	public function __ipow__(unused_other:Dynamic):Dynamic;
	public function __irealdiv__(unused_other:Dynamic):Dynamic;
	public function __isub__(unused_other:Dynamic):Dynamic;
	/**
		Dummy method to prevent iteration. Do not call.
		
		NOTE(mrry): If we register __getitem__ as an overloaded operator,
		Python will valiantly attempt to iterate over the variable's Tensor from 0
		to infinity.  Declaring this method prevents this unintended behavior.
		
		Raises:
		  TypeError: when invoked.
	**/
	public function __iter__():Dynamic;
	public function __itruediv__(unused_other:Dynamic):Dynamic;
	/**
		Returns the truth value of (x <= y) element-wise.
		
		*NOTE*: `math.less_equal` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __le__(x:Dynamic, y:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Returns the truth value of (x < y) element-wise.
		
		*NOTE*: `math.less` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __lt__(x:Dynamic, y:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Multiplies matrix `a` by matrix `b`, producing `a` * `b`.
		
		The inputs must, following any transpositions, be tensors of rank >= 2
		where the inner 2 dimensions specify valid matrix multiplication arguments,
		and any further outer dimensions match.
		
		Both matrices must be of the same type. The supported types are:
		`float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.
		
		Either matrix can be transposed or adjointed (conjugated and transposed) on
		the fly by setting one of the corresponding flag to `True`. These are `False`
		by default.
		
		If one or both of the matrices contain a lot of zeros, a more efficient
		multiplication algorithm can be used by setting the corresponding
		`a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.
		This optimization is only available for plain matrices (rank-2 tensors) with
		datatypes `bfloat16` or `float32`.
		
		For example:
		
		```python
		# 2-D tensor `a`
		# [[1, 2, 3],
		#  [4, 5, 6]]
		a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
		
		# 2-D tensor `b`
		# [[ 7,  8],
		#  [ 9, 10],
		#  [11, 12]]
		b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
		
		# `a` * `b`
		# [[ 58,  64],
		#  [139, 154]]
		c = tf.matmul(a, b)
		
		
		# 3-D tensor `a`
		# [[[ 1,  2,  3],
		#   [ 4,  5,  6]],
		#  [[ 7,  8,  9],
		#   [10, 11, 12]]]
		a = tf.constant(np.arange(1, 13, dtype=np.int32),
		                shape=[2, 2, 3])
		
		# 3-D tensor `b`
		# [[[13, 14],
		#   [15, 16],
		#   [17, 18]],
		#  [[19, 20],
		#   [21, 22],
		#   [23, 24]]]
		b = tf.constant(np.arange(13, 25, dtype=np.int32),
		                shape=[2, 3, 2])
		
		# `a` * `b`
		# [[[ 94, 100],
		#   [229, 244]],
		#  [[508, 532],
		#   [697, 730]]]
		c = tf.matmul(a, b)
		
		# Since python >= 3.5 the @ operator is supported (see PEP 465).
		# In TensorFlow, it simply calls the `tf.matmul()` function, so the
		# following lines are equivalent:
		d = a @ b @ [[10.], [11.]]
		d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
		```
		
		Args:
		  a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,
		    `complex128` and rank > 1.
		  b: `Tensor` with same type and rank as `a`.
		  transpose_a: If `True`, `a` is transposed before multiplication.
		  transpose_b: If `True`, `b` is transposed before multiplication.
		  adjoint_a: If `True`, `a` is conjugated and transposed before
		    multiplication.
		  adjoint_b: If `True`, `b` is conjugated and transposed before
		    multiplication.
		  a_is_sparse: If `True`, `a` is treated as a sparse matrix.
		  b_is_sparse: If `True`, `b` is treated as a sparse matrix.
		  name: Name for the operation (optional).
		
		Returns:
		  A `Tensor` of the same type as `a` and `b` where each inner-most matrix is
		  the product of the corresponding matrices in `a` and `b`, e.g. if all
		  transpose or adjoint attributes are `False`:
		
		  `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),
		  for all indices i, j.
		
		  Note: This is matrix product, not element-wise product.
		
		
		Raises:
		  ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b
		    are both set to True.
	**/
	static public function __matmul__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		Returns element-wise remainder of division. When `x < 0` xor `y < 0` is
		
		true, this follows Python semantics in that the result here is consistent
		with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.
		
		*NOTE*: `floormod` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __mod__(x:Dynamic, y:Dynamic):Dynamic;
	static public var __module__ : Dynamic;
	/**
		Dispatches cwise mul for "Dense*Dense" and "Dense*Sparse".
	**/
	static public function __mul__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		Return self!=value.
	**/
	public function __ne__(value:Dynamic):Dynamic;
	/**
		Computes numerical negative value element-wise.
		
		I.e., \\(y = -x\\).
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __neg__(x:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Create and return a new object.  See help(type) for accurate signature.
	**/
	static public function __new__(?args:python.VarArgs<Dynamic>, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
	public function __nonzero__():Dynamic;
	/**
		Returns the truth value of x OR y element-wise.
		
		*NOTE*: `math.logical_or` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor` of type `bool`.
		  y: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __or__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		Computes the power of one value to another.
		
		Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for
		corresponding elements in `x` and `y`. For example:
		
		```python
		x = tf.constant([[2, 2], [3, 3]])
		y = tf.constant([[8, 16], [2, 3]])
		tf.pow(x, y)  # [[256, 65536], [9, 27]]
		```
		
		Args:
		  x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,
		   `complex64`, or `complex128`.
		  y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,
		   `complex64`, or `complex128`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`.
	**/
	static public function __pow__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		Returns x + y element-wise.
		
		*NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __radd__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Returns the truth value of x AND y element-wise.
		
		*NOTE*: `math.logical_and` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor` of type `bool`.
		  y: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __rand__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Divide two values using Python 2 semantics. Used for Tensor.__div__.
		
		Args:
		  x: `Tensor` numerator of real numeric type.
		  y: `Tensor` denominator of real numeric type.
		  name: A name for the operation (optional).
		Returns:
		  `x / y` returns the quotient of x and y.
	**/
	static public function __rdiv__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		helper for pickle
	**/
	public function __reduce__():Dynamic;
	/**
		helper for pickle
	**/
	public function __reduce_ex__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Return repr(self).
	**/
	public function __repr__():Dynamic;
	/**
		Divides `x / y` elementwise, rounding toward the most negative integer.
		
		The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for
		floating point arguments so that the result is always an integer (though
		possibly an integer represented as floating point).  This op is generated by
		`x // y` floor division in Python 3 and in Python 2.7 with
		`from __future__ import division`.
		
		`x` and `y` must have the same type, and the result will have the same type
		as well.
		
		Args:
		  x: `Tensor` numerator of real numeric type.
		  y: `Tensor` denominator of real numeric type.
		  name: A name for the operation (optional).
		
		Returns:
		  `x / y` rounded down.
		
		Raises:
		  TypeError: If the inputs are complex.
	**/
	static public function __rfloordiv__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Multiplies matrix `a` by matrix `b`, producing `a` * `b`.
		
		The inputs must, following any transpositions, be tensors of rank >= 2
		where the inner 2 dimensions specify valid matrix multiplication arguments,
		and any further outer dimensions match.
		
		Both matrices must be of the same type. The supported types are:
		`float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.
		
		Either matrix can be transposed or adjointed (conjugated and transposed) on
		the fly by setting one of the corresponding flag to `True`. These are `False`
		by default.
		
		If one or both of the matrices contain a lot of zeros, a more efficient
		multiplication algorithm can be used by setting the corresponding
		`a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.
		This optimization is only available for plain matrices (rank-2 tensors) with
		datatypes `bfloat16` or `float32`.
		
		For example:
		
		```python
		# 2-D tensor `a`
		# [[1, 2, 3],
		#  [4, 5, 6]]
		a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
		
		# 2-D tensor `b`
		# [[ 7,  8],
		#  [ 9, 10],
		#  [11, 12]]
		b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
		
		# `a` * `b`
		# [[ 58,  64],
		#  [139, 154]]
		c = tf.matmul(a, b)
		
		
		# 3-D tensor `a`
		# [[[ 1,  2,  3],
		#   [ 4,  5,  6]],
		#  [[ 7,  8,  9],
		#   [10, 11, 12]]]
		a = tf.constant(np.arange(1, 13, dtype=np.int32),
		                shape=[2, 2, 3])
		
		# 3-D tensor `b`
		# [[[13, 14],
		#   [15, 16],
		#   [17, 18]],
		#  [[19, 20],
		#   [21, 22],
		#   [23, 24]]]
		b = tf.constant(np.arange(13, 25, dtype=np.int32),
		                shape=[2, 3, 2])
		
		# `a` * `b`
		# [[[ 94, 100],
		#   [229, 244]],
		#  [[508, 532],
		#   [697, 730]]]
		c = tf.matmul(a, b)
		
		# Since python >= 3.5 the @ operator is supported (see PEP 465).
		# In TensorFlow, it simply calls the `tf.matmul()` function, so the
		# following lines are equivalent:
		d = a @ b @ [[10.], [11.]]
		d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
		```
		
		Args:
		  a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,
		    `complex128` and rank > 1.
		  b: `Tensor` with same type and rank as `a`.
		  transpose_a: If `True`, `a` is transposed before multiplication.
		  transpose_b: If `True`, `b` is transposed before multiplication.
		  adjoint_a: If `True`, `a` is conjugated and transposed before
		    multiplication.
		  adjoint_b: If `True`, `b` is conjugated and transposed before
		    multiplication.
		  a_is_sparse: If `True`, `a` is treated as a sparse matrix.
		  b_is_sparse: If `True`, `b` is treated as a sparse matrix.
		  name: Name for the operation (optional).
		
		Returns:
		  A `Tensor` of the same type as `a` and `b` where each inner-most matrix is
		  the product of the corresponding matrices in `a` and `b`, e.g. if all
		  transpose or adjoint attributes are `False`:
		
		  `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),
		  for all indices i, j.
		
		  Note: This is matrix product, not element-wise product.
		
		
		Raises:
		  ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b
		    are both set to True.
	**/
	static public function __rmatmul__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Returns element-wise remainder of division. When `x < 0` xor `y < 0` is
		
		true, this follows Python semantics in that the result here is consistent
		with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.
		
		*NOTE*: `floormod` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __rmod__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Dispatches cwise mul for "Dense*Dense" and "Dense*Sparse".
	**/
	static public function __rmul__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Returns the truth value of x OR y element-wise.
		
		*NOTE*: `math.logical_or` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor` of type `bool`.
		  y: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __ror__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Computes the power of one value to another.
		
		Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for
		corresponding elements in `x` and `y`. For example:
		
		```python
		x = tf.constant([[2, 2], [3, 3]])
		y = tf.constant([[8, 16], [2, 3]])
		tf.pow(x, y)  # [[256, 65536], [9, 27]]
		```
		
		Args:
		  x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,
		   `complex64`, or `complex128`.
		  y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,
		   `complex64`, or `complex128`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`.
	**/
	static public function __rpow__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Returns x - y element-wise.
		
		*NOTE*: `Subtract` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __rsub__(y:Dynamic, x:Dynamic):Dynamic;
	static public function __rtruediv__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		x ^ y = (x | y) & ~(x & y).
	**/
	static public function __rxor__(y:Dynamic, x:Dynamic):Dynamic;
	/**
		Implement setattr(self, name, value).
	**/
	public function __setattr__(name:Dynamic, value:Dynamic):Dynamic;
	/**
		__sizeof__() -> int
		size of object in memory, in bytes
	**/
	public function __sizeof__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Return str(self).
	**/
	public function __str__():Dynamic;
	/**
		Returns x - y element-wise.
		
		*NOTE*: `Subtract` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __sub__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		Abstract classes can override this to customize issubclass().
		
		This is invoked early on by abc.ABCMeta.__subclasscheck__().
		It should return True, False or NotImplemented.  If it returns
		NotImplemented, the normal algorithm is used.  Otherwise, it
		overrides the normal algorithm (and the outcome is cached).
	**/
	public function __subclasshook__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	static public function __truediv__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		list of weak references to the object (if defined)
	**/
	public var __weakref__ : Dynamic;
	/**
		x ^ y = (x | y) & ~(x & y).
	**/
	static public function __xor__(x:Dynamic, y:Dynamic):Dynamic;
	/**
		Restore-on-create for a variable be saved with this `Checkpointable`.
		
		If the user has requested that this object or another `Checkpointable` which
		depends on this object be restored from a checkpoint (deferred loading
		before variable object creation), `initializer` may be ignored and the value
		from the checkpoint used instead.
		
		Args:
		  name: A name for the variable. Must be unique within this object.
		  shape: The shape of the variable.
		  dtype: The data type of the variable.
		  initializer: The initializer to use. Ignored if there is a deferred
		    restoration left over from a call to
		    `_restore_from_checkpoint_position`.
		  getter: The getter to wrap which actually fetches the variable.
		  overwrite: If True, disables unique name and type checks.
		  **kwargs_for_getter: Passed to the getter.
		
		Returns:
		  The new variable object.
		
		Raises:
		  ValueError: If the variable name is not unique.
	**/
	public function _add_variable_with_custom_getter(name:Dynamic, ?shape:Dynamic, ?dtype:Dynamic, ?initializer:Dynamic, ?getter:Dynamic, ?overwrite:Dynamic, ?kwargs_for_getter:python.KwArgs<Dynamic>):Dynamic;
	/**
		Conversion function for Graph.as_graph_element().
	**/
	public function _as_graph_element():Dynamic;
	/**
		Makes assignments depend on the cached value, if any.
		
		This prevents undefined behavior with reads not ordered wrt writes.
		
		Yields:
		  None.
	**/
	public function _assign_dependencies():Dynamic;
	/**
		All dependencies of this object.
		
		May be overridden to include conditional dependencies.
		
		Returns:
		  A list of `CheckpointableReference` objects indicating named
		  `Checkpointable` dependencies which should be saved along with this
		  object.
	**/
	public var _checkpoint_dependencies : Dynamic;
	/**
		A dictionary with deferred dependencies.
		
		Stores restorations for other Checkpointable objects on which this object
		may eventually depend. May be overridden by sub-classes (e.g. Optimizers use
		conditional dependencies based the current graph, and so need separate
		management of deferred dependencies too).
		
		Returns:
		  A dictionary mapping from local name to a list of _CheckpointPosition
		  objects.
	**/
	public var _deferred_dependencies : Dynamic;
	public function _dense_var_to_tensor(?dtype:Dynamic, ?name:Dynamic, ?as_ref:Dynamic):Dynamic;
	/**
		Find the initialized value for a variable op.
		
		To do so, lookup the variable op in the variables collection.
		
		Args:
		  variable_op: A variable `Operation`.
		Returns:
		  A `Tensor` representing the initialized value for the variable or `None`
		  if the initialized value could not be found.
	**/
	public function _find_initialized_value_for_variable(variable_op:Dynamic):Dynamic;
	/**
		For implementing `Checkpointable`. This object is saveable on its own.
	**/
	public function _gather_saveables_for_checkpoint():Dynamic;
	public function _get_save_slice_info():Dynamic;
	/**
		Pop and load any deferred checkpoint restores into `checkpointable`.
		
		This method does not add a new dependency on `checkpointable`, but it does
		check if any outstanding/deferred dependencies have been queued waiting for
		this dependency to be added (matched based on `name`). If so,
		`checkpointable` and its dependencies are restored. The restorations are
		considered fulfilled and so are deleted.
		
		`_track_checkpointable` is more appropriate for adding a
		normal/unconditional dependency, and includes handling for deferred
		restorations. This method allows objects such as `Optimizer` to use the same
		restoration logic while managing conditional dependencies themselves, by
		overriding `_checkpoint_dependencies` and `_lookup_dependency` to change the
		object's dependencies based on the context it is saved/restored in (a single
		optimizer instance can have state associated with multiple graphs).
		
		Args:
		  name: The name of the dependency within this object (`self`), used to
		    match `checkpointable` with values saved in a checkpoint.
		  checkpointable: The Checkpointable object to restore (inheriting from
		    `CheckpointableBase`).
	**/
	public function _handle_deferred_dependencies(name:Dynamic, checkpointable:Dynamic):Dynamic;
	/**
		Creates a variable.
		
		Args:
		  initial_value: A `Tensor`, or Python object convertible to a `Tensor`,
		    which is the initial value for the Variable. The initial value must have
		    a shape specified unless `validate_shape` is set to False. Can also be a
		    callable with no argument that returns the initial value when called.
		    (Note that initializer functions from init_ops.py must first be bound
		     to a shape before being used here.)
		  trainable: If `True`, the default, also adds the variable to the graph
		    collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as
		    the default list of variables to use by the `Optimizer` classes.
		  collections: List of graph collections keys. The new variable is added to
		    these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.
		  validate_shape: Ignored. Provided for compatibility with tf.Variable.
		  caching_device: Optional device string or function describing where the
		    Variable should be cached for reading.  Defaults to the Variable's
		    device.  If not `None`, caches on another device.  Typical use is to
		    cache on the device where the Ops using the Variable reside, to
		    deduplicate copying through `Switch` and other conditional statements.
		  name: Optional name for the variable. Defaults to `'Variable'` and gets
		    uniquified automatically.
		  dtype: If set, initial_value will be converted to the given type.
		    If None, either the datatype will be kept (if initial_value is
		   a Tensor) or float32 will be used (if it is a Python object convertible
		   to a Tensor).
		  constraint: An optional projection function to be applied to the variable
		    after being updated by an `Optimizer` (e.g. used to implement norm
		    constraints or value constraints for layer weights). The function must
		    take as input the unprojected Tensor representing the value of the
		    variable and return the Tensor for the projected value
		    (which must have the same shape). Constraints are not safe to
		    use when doing asynchronous distributed training.
		
		Raises:
		  ValueError: If the initial value is not specified, or does not have a
		    shape and `validate_shape` is `True`.
		
		@compatibility(eager)
		When Eager Execution is enabled, variables are never added to collections.
		It is not implicitly added to the `GLOBAL_VARIABLES` or
		`TRAINABLE_VARIABLES` collections, and the `collections` argument is
		ignored.
		@end_compatibility
	**/
	public function _init_from_args(?initial_value:Dynamic, ?trainable:Dynamic, ?collections:Dynamic, ?validate_shape:Dynamic, ?caching_device:Dynamic, ?name:Dynamic, ?dtype:Dynamic, ?constraint:Dynamic):Dynamic;
	/**
		Initializes from `VariableDef` proto.
	**/
	public function _init_from_proto(variable_def:Dynamic, ?import_scope:Dynamic):Dynamic;
	public function _lazy_read(op:Dynamic):Dynamic;
	/**
		Look up a dependency by name.
		
		May be overridden to include conditional dependencies.
		
		Args:
		  name: The local name of the dependency.
		Returns:
		  A `Checkpointable` object, or `None` if no dependency by this name was
		  found.
	**/
	public function _lookup_dependency(name:Dynamic):Dynamic;
	/**
		Initialize dependency management.
		
		Not __init__, since most objects will forget to call it.
	**/
	public function _maybe_initialize_checkpointable():Dynamic;
	/**
		Restore the object's attributes from a name-based checkpoint.
	**/
	public function _name_based_attribute_restore(checkpoint:Dynamic):Dynamic;
	/**
		If automatic dependency tracking is enabled, ignores `value`.
	**/
	public function _no_dependency(value:Dynamic):Dynamic;
	/**
		Return a dependency's value for restore-on-create.
		
		Note the restoration is not deleted; if for some reason preload is called
		and then not assigned to the variable (for example because a custom getter
		overrides the initializer), the assignment will still happen once the
		variable is tracked (determined based on checkpoint.restore_uid).
		
		Args:
		  name: The object-local name of the dependency holding the variable's
		    value.
		  shape: The shape of the variable being loaded into.
		Returns:
		  An callable for use as a variable's initializer/initial_value, or None if
		  one should not be set (either because there was no variable with this name
		  in the checkpoint or because it needs more complex deserialization). Any
		  non-trivial deserialization will happen when the variable object is
		  tracked.
	**/
	public function _preload_simple_restoration(name:Dynamic, shape:Dynamic):Dynamic;
	public function _read_variable_op():Dynamic;
	/**
		Unsupported.
	**/
	public function _ref():Dynamic;
	/**
		Restore this object and its dependencies (may be deferred).
	**/
	public function _restore_from_checkpoint_position(checkpoint_position:Dynamic):Dynamic;
	/**
		Replace dependencies on variables with their initialized values.
		
		Args:
		  op: An `Operation`. The operation to replace.
		  op_cache: A dict mapping operation names to `Operation`s. Used to memoize
		    the results so as to avoid creating redundant operations.
		Returns:
		  An `Operation` compatible with `op`. Any inputs that lead to variable
		  values will be replaced with a corresponding graph that uses the
		  variable's initialized values. This is done on a best-effort basis. If no
		  modifications need to be made then `op` will be returned unchanged.
	**/
	public function _safe_initial_value_from_op(op:Dynamic, op_cache:Dynamic):Dynamic;
	/**
		Replace dependencies on variables with their initialized values.
		
		Args:
		  tensor: A `Tensor`. The tensor to replace.
		  op_cache: A dict mapping operation names to `Operation`s. Used to memoize
		    the results so as to avoid creating redundant operations.
		Returns:
		  A `Tensor` compatible with `tensor`. Any inputs that lead to variable
		  values will be replaced with a corresponding graph that uses the
		  variable's initialized values. This is done on a best-effort basis. If no
		  modifications need to be made then `tensor` will be returned unchanged.
	**/
	public function _safe_initial_value_from_tensor(tensor:Dynamic, op_cache:Dynamic):Dynamic;
	/**
		Sets the slice info for this `ResourceVariable`.
		
		Args:
		  save_slice_info: A `Variable.SaveSliceInfo` object.
	**/
	public function _set_save_slice_info(save_slice_info:Dynamic):Dynamic;
	public function _shape_as_list():Dynamic;
	public function _shape_tuple():Dynamic;
	/**
		The shared name of the variable.
		
		  Unlike name(), shared_name doesn't have ":0" suffix. It is user-specified
		  name with name scope prefix.
		
		Returns:
		  variable name.
	**/
	public var _shared_name : Dynamic;
	/**
		To pass resource_variable_ops.is_resource_variable check.
	**/
	public function _should_act_as_resource_variable():Dynamic;
	/**
		Restore this object, and either queue its dependencies or defer them.
	**/
	public function _single_restoration_from_checkpoint_position(checkpoint_position:Dynamic, visit_queue:Dynamic):Dynamic;
	public function _strided_slice_assign(begin:Dynamic, end:Dynamic, strides:Dynamic, value:Dynamic, name:Dynamic, begin_mask:Dynamic, end_mask:Dynamic, ellipsis_mask:Dynamic, new_axis_mask:Dynamic, shrink_axis_mask:Dynamic):Dynamic;
	static public var _tf_api_names : Dynamic;
	static public var _tf_api_names_v1 : Dynamic;
	/**
		Declare a dependency on another `Checkpointable` object.
		
		Indicates that checkpoints for this object should include variables from
		`checkpointable`.
		
		Variables in a checkpoint are mapped to `Checkpointable`s based on the names
		provided when the checkpoint was written. To avoid breaking existing
		checkpoints when modifying a class, neither variable names nor dependency
		names (the names passed to `_track_checkpointable`) may change.
		
		Args:
		  checkpointable: A `Checkpointable` which this object depends on.
		  name: A local name for `checkpointable`, used for loading checkpoints into
		    the correct objects.
		  overwrite: Boolean, whether silently replacing dependencies is OK. Used
		    for __setattr__, where throwing an error on attribute reassignment would
		    be inappropriate.
		
		Returns:
		  `checkpointable`, for convenience when declaring a dependency and
		  assigning to a member variable in one statement.
		
		Raises:
		  TypeError: If `checkpointable` does not inherit from `Checkpointable`.
		  ValueError: If another object is already tracked by this name.
	**/
	public function _track_checkpointable(checkpointable:Dynamic, name:Dynamic, ?overwrite:Dynamic):Dynamic;
	/**
		Attempt to guard against dependencies on uninitialized variables.
		
		Replace references to variables in `initial_value` with references to the
		variable's initialized values. The initialized values are essentially
		conditional TensorFlow graphs that return a variable's value if it is
		initialized or its `initial_value` if it hasn't been initialized. This
		replacement is done on a best effort basis:
		
		- If the `initial_value` graph contains cycles, we don't do any
		  replacements for that graph.
		- If the variables that `initial_value` depends on are not present in the
		  `GLOBAL_VARIABLES` or `LOCAL_VARIABLES` we don't replace them.
		
		In these cases, it is up to the caller to ensure that the `initial_value`
		graph uses initialized variables or that they guard access to variables
		using their `initialized_value` method.
		
		Args:
		  initial_value: `Tensor`. The initial value.
		Returns:
		  A `Tensor` suitable to initialize a variable.
		Raises:
		  TypeError: If `initial_value` is not a `Tensor`.
	**/
	public function _try_guard_against_uninitialized_dependencies(initial_value:Dynamic):Dynamic;
	/**
		Assigns a new value to this variable.
		
		Args:
		  value: A `Tensor`. The new value for this variable.
		  use_locking: If `True`, use locking during the assignment.
		  name: The name to use for the assignment.
		  read_value: A `bool`. Whether to read and return the new value of the
		      variable or not.
		
		Returns:
		  If `read_value` is `True`, this method will return the new value of the
		  variable after the assignment has completed. Otherwise, when in graph mode
		  it will return the `Operation` that does the assignment, and when in eager
		  mode it will return `None`.
	**/
	public function assign(value:Dynamic, ?use_locking:Dynamic, ?name:Dynamic, ?read_value:Dynamic):Dynamic;
	/**
		Adds a value to this variable.
		
		Args:
		  delta: A `Tensor`. The value to add to this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: The name to use for the operation.
		  read_value: A `bool`. Whether to read and return the new value of the
		      variable or not.
		
		Returns:
		  If `read_value` is `True`, this method will return the new value of the
		  variable after the assignment has completed. Otherwise, when in graph mode
		  it will return the `Operation` that does the assignment, and when in eager
		  mode it will return `None`.
	**/
	public function assign_add(delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic, ?read_value:Dynamic):Dynamic;
	/**
		Subtracts a value from this variable.
		
		Args:
		  delta: A `Tensor`. The value to subtract from this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: The name to use for the operation.
		  read_value: A `bool`. Whether to read and return the new value of the
		      variable or not.
		
		Returns:
		  If `read_value` is `True`, this method will return the new value of the
		  variable after the assignment has completed. Otherwise, when in graph mode
		  it will return the `Operation` that does the assignment, and when in eager
		  mode it will return `None`.
	**/
	public function assign_sub(delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic, ?read_value:Dynamic):Dynamic;
	/**
		Assigns `IndexedSlices` to this variable batch-wise.
		
		Analogous to `batch_gather`. This assumes that this variable and the
		sparse_delta IndexedSlices have a series of leading dimensions that are the
		same for all of them, and the updates are performed on the last dimension of
		indices. In other words, the dimensions should be the following:
		
		`num_prefix_dims = sparse_delta.indices.ndims - 1`
		`batch_dim = num_prefix_dims + 1`
		`sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[
		     batch_dim:]`
		
		where
		
		`sparse_delta.updates.shape[:num_prefix_dims]`
		`== sparse_delta.indices.shape[:num_prefix_dims]`
		`== var.shape[:num_prefix_dims]`
		
		And the operation performed can be expressed as:
		
		`var[i_1, ..., i_n,
		     sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[
		        i_1, ..., i_n, j]`
		
		When sparse_delta.indices is a 1D tensor, this operation is equivalent to
		`scatter_update`.
		
		To avoid this operation one can looping over the first `ndims` of the
		variable and using `scatter_update` on the subtensors that result of slicing
		the first dimension. This is a valid option for `ndims = 1`, but less
		efficient than this implementation.
		
		Args:
		  sparse_delta: `IndexedSlices` to be assigned to this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function batch_scatter_update(sparse_delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Returns the constraint function associated with this variable.
		
		Returns:
		  The constraint function that was passed to the variable constructor.
		  Can be `None` if no constraint was passed.
	**/
	public var constraint : Dynamic;
	/**
		Increments this variable until it reaches `limit`. (deprecated)
		
		Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
		Instructions for updating:
		Prefer Dataset.range instead.
		
		When that Op is run it tries to increment the variable by `1`. If
		incrementing the variable would bring it above `limit` then the Op raises
		the exception `OutOfRangeError`.
		
		If no error is raised, the Op outputs the value of the variable before
		the increment.
		
		This is essentially a shortcut for `count_up_to(self, limit)`.
		
		Args:
		  limit: value at which incrementing the variable raises an error.
		
		Returns:
		  A `Tensor` that will hold the variable value before the increment. If no
		  other Op modifies this variable, the values produced will all be
		  distinct.
	**/
	public function count_up_to(limit:Dynamic):Dynamic;
	/**
		The op responsible for initializing this variable.
	**/
	public var create : Dynamic;
	/**
		The device this variable is on.
	**/
	public var device : Dynamic;
	/**
		The dtype of this variable.
	**/
	public var dtype : Dynamic;
	/**
		Evaluates and returns the value of this variable.
	**/
	public function eval(?session:Dynamic):Dynamic;
	/**
		Returns a `Variable` object created from `variable_def`.
	**/
	static public function from_proto(variable_def:Dynamic, ?import_scope:Dynamic):Dynamic;
	/**
		Alias of Variable.shape.
	**/
	public function get_shape():Dynamic;
	/**
		The `Graph` of this variable.
	**/
	public var graph : Dynamic;
	/**
		The handle by which this variable can be accessed.
	**/
	public var handle : Dynamic;
	/**
		Returns the Tensor used as the initial value for the variable.
	**/
	public var initial_value : Dynamic;
	/**
		Returns the value of the initialized variable.
		
		You should use this instead of the variable itself to initialize another
		variable with a value that depends on the value of this variable.
		
		```python
		# Initialize 'v' with a random tensor.
		v = tf.Variable(tf.truncated_normal([10, 40]))
		# Use `initialized_value` to guarantee that `v` has been
		# initialized before its value is used to initialize `w`.
		# The random values are picked only once.
		w = tf.Variable(v.initialized_value() * 2.0)
		```
		
		Returns:
		  A `Tensor` holding the value of this variable after its initializer
		  has run.
	**/
	public function initialized_value():Dynamic;
	/**
		The op responsible for initializing this variable.
	**/
	public var initializer : Dynamic;
	/**
		Checks whether a resource variable has been initialized.
		
		Outputs boolean scalar indicating whether the tensor has been initialized.
		
		Args:
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	public function is_initialized(?name:Dynamic):Dynamic;
	/**
		Load new value into this variable.
		
		Writes new value to variable's memory. Doesn't add ops to the graph.
		
		This convenience method requires a session where the graph
		containing this variable has been launched. If no session is
		passed, the default session is used.  See `tf.Session` for more
		information on launching a graph and on sessions.
		
		```python
		v = tf.Variable([1, 2])
		init = tf.global_variables_initializer()
		
		with tf.Session() as sess:
		    sess.run(init)
		    # Usage passing the session explicitly.
		    v.load([2, 3], sess)
		    print(v.eval(sess)) # prints [2 3]
		    # Usage with the default session.  The 'with' block
		    # above makes 'sess' the default session.
		    v.load([3, 4], sess)
		    print(v.eval()) # prints [3 4]
		```
		
		Args:
		    value: New variable value
		    session: The session to use to evaluate this variable. If
		      none, the default session is used.
		
		Raises:
		    ValueError: Session is not passed and no default session
	**/
	public function load(value:Dynamic, ?session:Dynamic):Dynamic;
	/**
		The name of the handle for this variable.
	**/
	public var name : Dynamic;
	public function numpy():Dynamic;
	/**
		The op for this variable.
	**/
	public var op : Dynamic;
	/**
		The dtype of the returned tensor when reading the var.
	**/
	public var read_dtype : Dynamic;
	/**
		Constructs an op which reads the value of this variable.
		
		Should be used when there are multiple reads, or when it is desirable to
		read the value only after some condition is true.
		
		Returns:
		 the read operation.
	**/
	public function read_value():Dynamic;
	/**
		Adds `IndexedSlices` from this variable.
		
		Args:
		  sparse_delta: `IndexedSlices` to be added to this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_add(sparse_delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Applies sparse addition to individual values or slices in a Variable.
		
		`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.
		
		`indices` must be integer tensor, containing indices into `ref`.
		It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.
		
		The innermost dimension of `indices` (with length `K`) corresponds to
		indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th
		dimension of `ref`.
		
		`updates` is `Tensor` of rank `Q-1+P-K` with shape:
		
		```
		[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].
		```
		
		For example, say we want to add 4 scattered elements to a rank-1 tensor to
		8 elements. In Python, that update would look like this:
		
		```python
		    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
		    indices = tf.constant([[4], [3], [1] ,[7]])
		    updates = tf.constant([9, 10, 11, 12])
		    add = ref.scatter_nd_add(indices, updates)
		    with tf.Session() as sess:
		      print sess.run(add)
		```
		
		The resulting update to ref would look like this:
		
		    [1, 13, 3, 14, 14, 6, 7, 20]
		
		See `tf.scatter_nd` for more details about how to make updates to
		slices.
		
		Args:
		  indices: The indices to be used in the operation.
		  updates: The values to be used in the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_nd_add(indices:Dynamic, updates:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Applies sparse subtraction to individual values or slices in a Variable.
		
		`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.
		
		`indices` must be integer tensor, containing indices into `ref`.
		It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.
		
		The innermost dimension of `indices` (with length `K`) corresponds to
		indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th
		dimension of `ref`.
		
		`updates` is `Tensor` of rank `Q-1+P-K` with shape:
		
		```
		[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].
		```
		
		For example, say we want to add 4 scattered elements to a rank-1 tensor to
		8 elements. In Python, that update would look like this:
		
		```python
		    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
		    indices = tf.constant([[4], [3], [1] ,[7]])
		    updates = tf.constant([9, 10, 11, 12])
		    op = ref.scatter_nd_sub(indices, updates)
		    with tf.Session() as sess:
		      print sess.run(op)
		```
		
		The resulting update to ref would look like this:
		
		    [1, -9, 3, -6, -6, 6, 7, -4]
		
		See `tf.scatter_nd` for more details about how to make updates to
		slices.
		
		Args:
		  indices: The indices to be used in the operation.
		  updates: The values to be used in the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_nd_sub(indices:Dynamic, updates:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Applies sparse assignment to individual values or slices in a Variable.
		
		`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.
		
		`indices` must be integer tensor, containing indices into `ref`.
		It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.
		
		The innermost dimension of `indices` (with length `K`) corresponds to
		indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th
		dimension of `ref`.
		
		`updates` is `Tensor` of rank `Q-1+P-K` with shape:
		
		```
		[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].
		```
		
		For example, say we want to add 4 scattered elements to a rank-1 tensor to
		8 elements. In Python, that update would look like this:
		
		```python
		    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
		    indices = tf.constant([[4], [3], [1] ,[7]])
		    updates = tf.constant([9, 10, 11, 12])
		    op = ref.scatter_nd_update(indices, updates)
		    with tf.Session() as sess:
		      print sess.run(op)
		```
		
		The resulting update to ref would look like this:
		
		    [1, 11, 3, 10, 9, 6, 7, 12]
		
		See `tf.scatter_nd` for more details about how to make updates to
		slices.
		
		Args:
		  indices: The indices to be used in the operation.
		  updates: The values to be used in the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_nd_update(indices:Dynamic, updates:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Subtracts `IndexedSlices` from this variable.
		
		Args:
		  sparse_delta: `IndexedSlices` to be subtracted from this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_sub(sparse_delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Assigns `IndexedSlices` to this variable.
		
		Args:
		  sparse_delta: `IndexedSlices` to be assigned to this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_update(sparse_delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Unsupported.
	**/
	public function set_shape(shape:Dynamic):Dynamic;
	/**
		The shape of this variable.
	**/
	public var shape : Dynamic;
	/**
		Reads the value of this variable sparsely, using `gather`.
	**/
	public function sparse_read(indices:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Converts a `ResourceVariable` to a `VariableDef` protocol buffer.
		
		Args:
		  export_scope: Optional `string`. Name scope to remove.
		
		Raises:
		  RuntimeError: If run in EAGER mode.
		
		Returns:
		  A `VariableDef` protocol buffer, or `None` if the `Variable` is not
		  in the specified name scope.
	**/
	public function to_proto(?export_scope:Dynamic):Dynamic;
	public var trainable : Dynamic;
	/**
		A cached operation which reads the value of this variable.
	**/
	public function value():Dynamic;
}